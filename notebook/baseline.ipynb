{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5e6ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước dữ liệu: (3042, 117)\n",
      "\n",
      "5 dòng đầu:\n",
      "                  object_id       Z  Z_err    EBV  target  u_n_obs  \\\n",
      "0  Dornhoth_fervain_onodrim  3.0490    NaN  0.110       0        5   \n",
      "1       Dornhoth_galadh_ylf  0.4324    NaN  0.058       0       15   \n",
      "2      Elrim_melethril_thul  0.4673    NaN  0.577       0        5   \n",
      "3        Ithil_tobas_rodwen  0.6946    NaN  0.012       0      108   \n",
      "4       Mirion_adar_Druadan  0.4161    NaN  0.058       0       13   \n",
      "\n",
      "   u_time_span  u_flux_mean  u_flux_std  u_flux_min  ...  color_mean_g_i  \\\n",
      "0     849.3841     0.584015    1.022628    0.000435  ...       -2.485906   \n",
      "1    2246.6157     0.027015    0.545088   -0.799254  ...       -0.173395   \n",
      "2     498.8939     0.006054    0.293108   -0.427984  ...       -2.060860   \n",
      "3    2858.4129     0.152980    0.439240   -1.455301  ...       -0.167220   \n",
      "4    1859.9219    -0.014067    0.406413   -0.828860  ...       -0.337740   \n",
      "\n",
      "   color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
      "0       -0.194878        -382.8829          44.0095         -44.0096   \n",
      "1       -0.252129        -821.6194          51.3512         -20.5406   \n",
      "2       -2.710143         444.6663           0.0000        -730.9880   \n",
      "3       -0.096281        1661.2019       -1331.8257        1331.8257   \n",
      "4       -0.086716         -48.5965        -234.1469        -108.2378   \n",
      "\n",
      "   t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
      "0         -13.2029         -17.6038        -338.8734          -0.0001   \n",
      "1         -64.1890         -20.5404        -770.2682          30.8106   \n",
      "2         110.6243        -147.4991         444.6663        -730.9880   \n",
      "3          25.7773       -1632.5606         329.3762           0.0000   \n",
      "4         474.9207        -523.5172        -282.7434        -342.3847   \n",
      "\n",
      "   t_peak_diff_r_z  \n",
      "0         -57.2125  \n",
      "1         -84.7296  \n",
      "2        -620.3637  \n",
      "3        1357.6030  \n",
      "4         366.6829  \n",
      "\n",
      "[5 rows x 117 columns]\n",
      "\n",
      "Thông tin cơ bản:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3042 entries, 0 to 3041\n",
      "Columns: 117 entries, object_id to t_peak_diff_r_z\n",
      "dtypes: float64(109), int64(7), object(1)\n",
      "memory usage: 2.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Đọc file CSV với pandas (khuyến nghị)\n",
    "df = pd.read_csv('/home/duy/Downloads/Mallorn/Mallorn/mallorn-astronomical-classification-challenge/train_features_ml.csv')\n",
    "print(\"Kích thước dữ liệu:\", df.shape)\n",
    "print(\"\\n5 dòng đầu:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nThông tin cơ bản:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586343fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRƯỚC KHI XỬ LÝ NaN\n",
      "============================================================\n",
      "Kích thước DataFrame: (2876, 117)\n",
      "\n",
      "Tổng số NaN trong mỗi cột:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Số hàng có ít nhất 1 NaN: 0\n",
      "Tổng số NaN trong toàn bộ DataFrame: 0\n",
      "\n",
      "============================================================\n",
      "XỬ LÝ NaN\n",
      "============================================================\n",
      "1. Điền giá trị 0 vào cột Z_err...\n",
      "   ✓ Đã điền 0 vào 0 giá trị NaN trong Z_err\n",
      "\n",
      "2. Xóa các hàng có NaN ở các cột khác...\n",
      "\n",
      "============================================================\n",
      "SAU KHI XỬ LÝ NaN\n",
      "============================================================\n",
      "Kích thước DataFrame sau khi xử lý: (2876, 117)\n",
      "Số hàng đã xóa: 0\n",
      "% hàng đã xóa: 0.00%\n",
      "\n",
      "Còn NaN trong DataFrame không? False\n",
      "Tổng số NaN còn lại: 0\n",
      "\n",
      "Thống kê cột Z_err sau khi điền 0:\n",
      "  - Min: 0.0\n",
      "  - Max: 0.0\n",
      "  - Mean: 0.0000\n",
      "  - Số giá trị = 0: 2876\n",
      "\n",
      "✓ DataFrame đã được cập nhật!\n",
      "✓ Kích thước hiện tại: (2876, 117)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHECK VÀ XỬ LÝ CÁC GIÁ TRỊ NaN\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRƯỚC KHI XỬ LÝ NaN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Kích thước DataFrame: {df.shape}\")\n",
    "print(f\"\\nTổng số NaN trong mỗi cột:\")\n",
    "nan_counts = df.isnull().sum()\n",
    "print(nan_counts[nan_counts > 0])\n",
    "\n",
    "print(f\"\\nSố hàng có ít nhất 1 NaN: {df.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Tổng số NaN trong toàn bộ DataFrame: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# ============================================================\n",
    "# ĐIỀN GIÁ TRỊ 0 VÀO CỘT Z_err\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XỬ LÝ NaN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Điền 0 vào cột Z_err\n",
    "print(\"1. Điền giá trị 0 vào cột Z_err...\")\n",
    "df['Z_err'] = df['Z_err'].fillna(0)\n",
    "print(f\"   ✓ Đã điền 0 vào {nan_counts['Z_err']} giá trị NaN trong Z_err\")\n",
    "\n",
    "# Xóa các hàng có NaN ở các cột khác (không phải Z_err)\n",
    "print(\"\\n2. Xóa các hàng có NaN ở các cột khác...\")\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAU KHI XỬ LÝ NaN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Kích thước DataFrame sau khi xử lý: {df_cleaned.shape}\")\n",
    "print(f\"Số hàng đã xóa: {len(df) - len(df_cleaned)}\")\n",
    "print(f\"% hàng đã xóa: {((len(df) - len(df_cleaned)) / len(df) * 100):.2f}%\")\n",
    "\n",
    "# Kiểm tra xem còn NaN không\n",
    "print(f\"\\nCòn NaN trong DataFrame không? {df_cleaned.isnull().any().any()}\")\n",
    "print(f\"Tổng số NaN còn lại: {df_cleaned.isnull().sum().sum()}\")\n",
    "\n",
    "# Hiển thị thống kê cột Z_err sau khi điền\n",
    "print(f\"\\nThống kê cột Z_err sau khi điền 0:\")\n",
    "print(f\"  - Min: {df_cleaned['Z_err'].min()}\")\n",
    "print(f\"  - Max: {df_cleaned['Z_err'].max()}\")\n",
    "print(f\"  - Mean: {df_cleaned['Z_err'].mean():.4f}\")\n",
    "print(f\"  - Số giá trị = 0: {(df_cleaned['Z_err'] == 0).sum()}\")\n",
    "\n",
    "# Cập nhật df với bản đã clean\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "print(\"\\n✓ DataFrame đã được cập nhật!\")\n",
    "print(f\"✓ Kích thước hiện tại: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6366d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột sẽ được scaling:\n",
      "['Z', 'EBV', 'u_n_obs', 'u_time_span', 'u_flux_mean', 'u_flux_std', 'u_flux_min', 'u_flux_max', 'u_flux_median', 'u_amplitude'] ...\n",
      "Tổng số cột cần scaling: 114\n",
      "\n",
      "Thống kê trước khi scaling (5 cột đầu):\n",
      "                 Z          EBV      u_n_obs  u_time_span  u_flux_mean\n",
      "count  2876.000000  2876.000000  2876.000000  2876.000000  2876.000000\n",
      "mean      0.682277     0.053392    13.598053  1852.156176     0.393870\n",
      "std       0.542379     0.054949    15.015810   557.017193     1.590099\n",
      "min       0.008771     0.002000     3.000000    28.247300   -17.135965\n",
      "25%       0.324500     0.021000     9.000000  1481.598725     0.002790\n",
      "50%       0.493800     0.037000    12.000000  1863.958150     0.137668\n",
      "75%       0.896825     0.067000    14.000000  2219.633650     0.372800\n",
      "max       4.924000     0.758000   161.000000  3525.770100    50.071001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Tạo bản sao của DataFrame để không ảnh hưởng đến dữ liệu gốc\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Xác định các cột cần scaling (bỏ qua cột tên bệnh nhân và label)\n",
    "columns_to_scale = [col for col in df_scaled.columns \n",
    "                   if col not in ['object_id', 'Z_err', 'target']]\n",
    "\n",
    "print(\"Các cột sẽ được scaling:\")\n",
    "print(columns_to_scale[:10], \"...\")  # Hiển thị 10 cột đầu\n",
    "print(f\"Tổng số cột cần scaling: {len(columns_to_scale)}\")\n",
    "\n",
    "# Hiển thị thống kê trước khi scaling\n",
    "print(\"\\nThống kê trước khi scaling (5 cột đầu):\")\n",
    "print(df_scaled[columns_to_scale[:5]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34604743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>Z</th>\n",
       "      <th>Z_err</th>\n",
       "      <th>EBV</th>\n",
       "      <th>target</th>\n",
       "      <th>u_n_obs</th>\n",
       "      <th>u_time_span</th>\n",
       "      <th>u_flux_mean</th>\n",
       "      <th>u_flux_std</th>\n",
       "      <th>u_flux_min</th>\n",
       "      <th>...</th>\n",
       "      <th>color_mean_g_i</th>\n",
       "      <th>color_mean_r_z</th>\n",
       "      <th>t_peak_diff_u_g</th>\n",
       "      <th>t_peak_diff_g_r</th>\n",
       "      <th>t_peak_diff_r_i</th>\n",
       "      <th>t_peak_diff_i_z</th>\n",
       "      <th>t_peak_diff_z_y</th>\n",
       "      <th>t_peak_diff_u_r</th>\n",
       "      <th>t_peak_diff_g_i</th>\n",
       "      <th>t_peak_diff_r_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dornhoth_fervain_onodrim</td>\n",
       "      <td>3.0490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>849.3841</td>\n",
       "      <td>0.584015</td>\n",
       "      <td>1.022628</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.485906</td>\n",
       "      <td>-0.194878</td>\n",
       "      <td>-382.8829</td>\n",
       "      <td>44.0095</td>\n",
       "      <td>-44.0096</td>\n",
       "      <td>-13.2029</td>\n",
       "      <td>-17.6038</td>\n",
       "      <td>-338.8734</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-57.2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dornhoth_galadh_ylf</td>\n",
       "      <td>0.4324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2246.6157</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.545088</td>\n",
       "      <td>-0.799254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173395</td>\n",
       "      <td>-0.252129</td>\n",
       "      <td>-821.6194</td>\n",
       "      <td>51.3512</td>\n",
       "      <td>-20.5406</td>\n",
       "      <td>-64.1890</td>\n",
       "      <td>-20.5404</td>\n",
       "      <td>-770.2682</td>\n",
       "      <td>30.8106</td>\n",
       "      <td>-84.7296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ithil_tobas_rodwen</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>2858.4129</td>\n",
       "      <td>0.152980</td>\n",
       "      <td>0.439240</td>\n",
       "      <td>-1.455301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167220</td>\n",
       "      <td>-0.096281</td>\n",
       "      <td>1661.2019</td>\n",
       "      <td>-1331.8257</td>\n",
       "      <td>1331.8257</td>\n",
       "      <td>25.7773</td>\n",
       "      <td>-1632.5606</td>\n",
       "      <td>329.3762</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1357.6030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mirion_adar_Druadan</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1859.9219</td>\n",
       "      <td>-0.014067</td>\n",
       "      <td>0.406413</td>\n",
       "      <td>-0.828860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337740</td>\n",
       "      <td>-0.086716</td>\n",
       "      <td>-48.5965</td>\n",
       "      <td>-234.1469</td>\n",
       "      <td>-108.2378</td>\n",
       "      <td>474.9207</td>\n",
       "      <td>-523.5172</td>\n",
       "      <td>-282.7434</td>\n",
       "      <td>-342.3847</td>\n",
       "      <td>366.6829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mirion_lalaith_neledh</td>\n",
       "      <td>1.1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1832.8097</td>\n",
       "      <td>0.422299</td>\n",
       "      <td>0.288031</td>\n",
       "      <td>-0.023856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394888</td>\n",
       "      <td>-0.449806</td>\n",
       "      <td>-99.0709</td>\n",
       "      <td>1183.0219</td>\n",
       "      <td>-1177.1941</td>\n",
       "      <td>1369.5080</td>\n",
       "      <td>-1427.7850</td>\n",
       "      <td>1083.9510</td>\n",
       "      <td>5.8278</td>\n",
       "      <td>192.3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>tinnu_gellui_tathar</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2189.5612</td>\n",
       "      <td>-0.068665</td>\n",
       "      <td>0.395604</td>\n",
       "      <td>-0.825168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309896</td>\n",
       "      <td>-0.412576</td>\n",
       "      <td>2085.4235</td>\n",
       "      <td>-595.4538</td>\n",
       "      <td>283.0408</td>\n",
       "      <td>-197.5945</td>\n",
       "      <td>-1746.3085</td>\n",
       "      <td>1489.9697</td>\n",
       "      <td>-312.4130</td>\n",
       "      <td>85.4463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>uir_heleg_corf</td>\n",
       "      <td>0.9598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2887.8277</td>\n",
       "      <td>1.134710</td>\n",
       "      <td>2.021232</td>\n",
       "      <td>-2.843739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342609</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-12.5968</td>\n",
       "      <td>25.1937</td>\n",
       "      <td>176.3559</td>\n",
       "      <td>-289.7275</td>\n",
       "      <td>-12.5968</td>\n",
       "      <td>12.5969</td>\n",
       "      <td>201.5496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>uir_rhosc_law</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1829.4130</td>\n",
       "      <td>0.041323</td>\n",
       "      <td>0.481292</td>\n",
       "      <td>-0.979264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361688</td>\n",
       "      <td>-0.500247</td>\n",
       "      <td>60.7013</td>\n",
       "      <td>-4.1863</td>\n",
       "      <td>-35.5835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0932</td>\n",
       "      <td>56.5150</td>\n",
       "      <td>-39.7698</td>\n",
       "      <td>-35.5835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>uruk_in_pess</td>\n",
       "      <td>1.1520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2568.6786</td>\n",
       "      <td>0.703543</td>\n",
       "      <td>1.046953</td>\n",
       "      <td>-2.024119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360625</td>\n",
       "      <td>-0.177830</td>\n",
       "      <td>-803.5525</td>\n",
       "      <td>732.9475</td>\n",
       "      <td>-396.7331</td>\n",
       "      <td>245.4366</td>\n",
       "      <td>581.6510</td>\n",
       "      <td>-70.6050</td>\n",
       "      <td>336.2144</td>\n",
       "      <td>-151.2965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>ylf_alph_mindon</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1719.8078</td>\n",
       "      <td>-0.189906</td>\n",
       "      <td>0.238451</td>\n",
       "      <td>-0.585085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232245</td>\n",
       "      <td>-0.032634</td>\n",
       "      <td>917.8616</td>\n",
       "      <td>-286.2403</td>\n",
       "      <td>21.2906</td>\n",
       "      <td>-2.3656</td>\n",
       "      <td>-26.0219</td>\n",
       "      <td>631.6213</td>\n",
       "      <td>-264.9497</td>\n",
       "      <td>18.9250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2876 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     object_id       Z  Z_err    EBV  target  u_n_obs  \\\n",
       "0     Dornhoth_fervain_onodrim  3.0490    0.0  0.110       0        5   \n",
       "1          Dornhoth_galadh_ylf  0.4324    0.0  0.058       0       15   \n",
       "3           Ithil_tobas_rodwen  0.6946    0.0  0.012       0      108   \n",
       "4          Mirion_adar_Druadan  0.4161    0.0  0.058       0       13   \n",
       "5        Mirion_lalaith_neledh  1.1970    0.0  0.054       0        8   \n",
       "...                        ...     ...    ...    ...     ...      ...   \n",
       "3037       tinnu_gellui_tathar  0.8898    0.0  0.042       0       16   \n",
       "3038            uir_heleg_corf  0.9598    0.0  0.042       0       13   \n",
       "3039             uir_rhosc_law  0.1543    0.0  0.024       0       16   \n",
       "3040              uruk_in_pess  1.1520    0.0  0.019       0       13   \n",
       "3041           ylf_alph_mindon  0.5595    0.0  0.034       0       17   \n",
       "\n",
       "      u_time_span  u_flux_mean  u_flux_std  u_flux_min  ...  color_mean_g_i  \\\n",
       "0        849.3841     0.584015    1.022628    0.000435  ...       -2.485906   \n",
       "1       2246.6157     0.027015    0.545088   -0.799254  ...       -0.173395   \n",
       "3       2858.4129     0.152980    0.439240   -1.455301  ...       -0.167220   \n",
       "4       1859.9219    -0.014067    0.406413   -0.828860  ...       -0.337740   \n",
       "5       1832.8097     0.422299    0.288031   -0.023856  ...       -0.394888   \n",
       "...           ...          ...         ...         ...  ...             ...   \n",
       "3037    2189.5612    -0.068665    0.395604   -0.825168  ...       -0.309896   \n",
       "3038    2887.8277     1.134710    2.021232   -2.843739  ...        0.342609   \n",
       "3039    1829.4130     0.041323    0.481292   -0.979264  ...       -0.361688   \n",
       "3040    2568.6786     0.703543    1.046953   -2.024119  ...        0.360625   \n",
       "3041    1719.8078    -0.189906    0.238451   -0.585085  ...       -0.232245   \n",
       "\n",
       "      color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
       "0          -0.194878        -382.8829          44.0095         -44.0096   \n",
       "1          -0.252129        -821.6194          51.3512         -20.5406   \n",
       "3          -0.096281        1661.2019       -1331.8257        1331.8257   \n",
       "4          -0.086716         -48.5965        -234.1469        -108.2378   \n",
       "5          -0.449806         -99.0709        1183.0219       -1177.1941   \n",
       "...              ...              ...              ...              ...   \n",
       "3037       -0.412576        2085.4235        -595.4538         283.0408   \n",
       "3038        0.074489           0.0000         -12.5968          25.1937   \n",
       "3039       -0.500247          60.7013          -4.1863         -35.5835   \n",
       "3040       -0.177830        -803.5525         732.9475        -396.7331   \n",
       "3041       -0.032634         917.8616        -286.2403          21.2906   \n",
       "\n",
       "      t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
       "0            -13.2029         -17.6038        -338.8734          -0.0001   \n",
       "1            -64.1890         -20.5404        -770.2682          30.8106   \n",
       "3             25.7773       -1632.5606         329.3762           0.0000   \n",
       "4            474.9207        -523.5172        -282.7434        -342.3847   \n",
       "5           1369.5080       -1427.7850        1083.9510           5.8278   \n",
       "...               ...              ...              ...              ...   \n",
       "3037        -197.5945       -1746.3085        1489.9697        -312.4130   \n",
       "3038         176.3559        -289.7275         -12.5968          12.5969   \n",
       "3039           0.0000          -2.0932          56.5150         -39.7698   \n",
       "3040         245.4366         581.6510         -70.6050         336.2144   \n",
       "3041          -2.3656         -26.0219         631.6213        -264.9497   \n",
       "\n",
       "      t_peak_diff_r_z  \n",
       "0            -57.2125  \n",
       "1            -84.7296  \n",
       "3           1357.6030  \n",
       "4            366.6829  \n",
       "5            192.3139  \n",
       "...               ...  \n",
       "3037          85.4463  \n",
       "3038         201.5496  \n",
       "3039         -35.5835  \n",
       "3040        -151.2965  \n",
       "3041          18.9250  \n",
       "\n",
       "[2876 rows x 117 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c97dacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Có NaN trong df không? False\n",
      "\n",
      "Số NaN trong mỗi cột:\n",
      "object_id          0\n",
      "Z                  0\n",
      "Z_err              0\n",
      "EBV                0\n",
      "target             0\n",
      "                  ..\n",
      "t_peak_diff_i_z    0\n",
      "t_peak_diff_z_y    0\n",
      "t_peak_diff_u_r    0\n",
      "t_peak_diff_g_i    0\n",
      "t_peak_diff_r_z    0\n",
      "Length: 117, dtype: int64\n",
      "\n",
      "% NaN trong mỗi cột:\n",
      "object_id          0.0\n",
      "Z                  0.0\n",
      "Z_err              0.0\n",
      "EBV                0.0\n",
      "target             0.0\n",
      "                  ... \n",
      "t_peak_diff_i_z    0.0\n",
      "t_peak_diff_z_y    0.0\n",
      "t_peak_diff_u_r    0.0\n",
      "t_peak_diff_g_i    0.0\n",
      "t_peak_diff_r_z    0.0\n",
      "Length: 117, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra xem có NaN không\n",
    "print(\"Có NaN trong df không?\", df.isnull().any().any())\n",
    "\n",
    "# Đếm số NaN trong mỗi cột\n",
    "print(\"\\nSố NaN trong mỗi cột:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Tính % NaN trong mỗi cột\n",
    "print(\"\\n% NaN trong mỗi cột:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88462019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaling đã hoàn thành!\n",
      "\n",
      "Thống kê sau khi scaling (5 cột đầu):\n",
      "                 Z          EBV      u_n_obs  u_time_span  u_flux_mean\n",
      "count  2876.000000  2876.000000  2876.000000  2876.000000  2876.000000\n",
      "mean      0.137024     0.067979     0.067076     0.521486     0.260834\n",
      "std       0.110347     0.072683     0.095037     0.159260     0.023660\n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%       0.064235     0.025132     0.037975     0.415537     0.255015\n",
      "50%       0.098679     0.046296     0.056962     0.524860     0.257021\n",
      "75%       0.180674     0.085979     0.069620     0.626554     0.260520\n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000\n",
      "\n",
      "Phạm vi giá trị sau scaling:\n",
      "Min: 0.000000\n",
      "Max: 1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scaling các cột đã chọn\n",
    "df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n",
    "\n",
    "print(\"Min-Max Scaling đã hoàn thành!\")\n",
    "print(\"\\nThống kê sau khi scaling (5 cột đầu):\")\n",
    "print(df_scaled[columns_to_scale[:5]].describe())\n",
    "\n",
    "# Kiểm tra phạm vi giá trị sau scaling\n",
    "print(f\"\\nPhạm vi giá trị sau scaling:\")\n",
    "print(f\"Min: {df_scaled[columns_to_scale].min().min():.6f}\")\n",
    "print(f\"Max: {df_scaled[columns_to_scale].max().max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06ec40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>Z</th>\n",
       "      <th>Z_err</th>\n",
       "      <th>EBV</th>\n",
       "      <th>target</th>\n",
       "      <th>u_n_obs</th>\n",
       "      <th>u_time_span</th>\n",
       "      <th>u_flux_mean</th>\n",
       "      <th>u_flux_std</th>\n",
       "      <th>u_flux_min</th>\n",
       "      <th>...</th>\n",
       "      <th>color_mean_g_i</th>\n",
       "      <th>color_mean_r_z</th>\n",
       "      <th>t_peak_diff_u_g</th>\n",
       "      <th>t_peak_diff_g_r</th>\n",
       "      <th>t_peak_diff_r_i</th>\n",
       "      <th>t_peak_diff_i_z</th>\n",
       "      <th>t_peak_diff_z_y</th>\n",
       "      <th>t_peak_diff_u_r</th>\n",
       "      <th>t_peak_diff_g_i</th>\n",
       "      <th>t_peak_diff_r_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dornhoth_fervain_onodrim</td>\n",
       "      <td>0.618533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.234777</td>\n",
       "      <td>0.263663</td>\n",
       "      <td>0.025473</td>\n",
       "      <td>0.701691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220216</td>\n",
       "      <td>0.430231</td>\n",
       "      <td>0.461601</td>\n",
       "      <td>0.625388</td>\n",
       "      <td>0.489657</td>\n",
       "      <td>0.481269</td>\n",
       "      <td>0.508584</td>\n",
       "      <td>0.501736</td>\n",
       "      <td>0.515684</td>\n",
       "      <td>0.472637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dornhoth_galadh_ylf</td>\n",
       "      <td>0.086187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.634268</td>\n",
       "      <td>0.255375</td>\n",
       "      <td>0.013209</td>\n",
       "      <td>0.684591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325172</td>\n",
       "      <td>0.426179</td>\n",
       "      <td>0.378973</td>\n",
       "      <td>0.626645</td>\n",
       "      <td>0.494262</td>\n",
       "      <td>0.471119</td>\n",
       "      <td>0.507978</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.521944</td>\n",
       "      <td>0.467464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ithil_tobas_rodwen</td>\n",
       "      <td>0.139531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013228</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.257249</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.670563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325453</td>\n",
       "      <td>0.437208</td>\n",
       "      <td>0.846568</td>\n",
       "      <td>0.389680</td>\n",
       "      <td>0.759607</td>\n",
       "      <td>0.489029</td>\n",
       "      <td>0.175839</td>\n",
       "      <td>0.635187</td>\n",
       "      <td>0.515684</td>\n",
       "      <td>0.738604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mirion_adar_Druadan</td>\n",
       "      <td>0.082871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.523706</td>\n",
       "      <td>0.254764</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.683958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317713</td>\n",
       "      <td>0.437885</td>\n",
       "      <td>0.524558</td>\n",
       "      <td>0.577734</td>\n",
       "      <td>0.477055</td>\n",
       "      <td>0.578441</td>\n",
       "      <td>0.404345</td>\n",
       "      <td>0.512945</td>\n",
       "      <td>0.446111</td>\n",
       "      <td>0.552323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mirion_lalaith_neledh</td>\n",
       "      <td>0.241744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.515954</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.701172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315120</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.515052</td>\n",
       "      <td>0.820523</td>\n",
       "      <td>0.267317</td>\n",
       "      <td>0.756529</td>\n",
       "      <td>0.218031</td>\n",
       "      <td>0.785879</td>\n",
       "      <td>0.516868</td>\n",
       "      <td>0.519544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>tinnu_gellui_tathar</td>\n",
       "      <td>0.179245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082278</td>\n",
       "      <td>0.617956</td>\n",
       "      <td>0.253951</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318977</td>\n",
       "      <td>0.414826</td>\n",
       "      <td>0.926462</td>\n",
       "      <td>0.515835</td>\n",
       "      <td>0.553827</td>\n",
       "      <td>0.444562</td>\n",
       "      <td>0.152402</td>\n",
       "      <td>0.866962</td>\n",
       "      <td>0.452201</td>\n",
       "      <td>0.499455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>uir_heleg_corf</td>\n",
       "      <td>0.193486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.817602</td>\n",
       "      <td>0.271857</td>\n",
       "      <td>0.051119</td>\n",
       "      <td>0.640874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348592</td>\n",
       "      <td>0.449292</td>\n",
       "      <td>0.533710</td>\n",
       "      <td>0.615690</td>\n",
       "      <td>0.503236</td>\n",
       "      <td>0.519005</td>\n",
       "      <td>0.452515</td>\n",
       "      <td>0.566894</td>\n",
       "      <td>0.518243</td>\n",
       "      <td>0.521281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>uir_rhosc_law</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082278</td>\n",
       "      <td>0.514983</td>\n",
       "      <td>0.255588</td>\n",
       "      <td>0.011571</td>\n",
       "      <td>0.680742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316626</td>\n",
       "      <td>0.408622</td>\n",
       "      <td>0.545142</td>\n",
       "      <td>0.617131</td>\n",
       "      <td>0.491311</td>\n",
       "      <td>0.483898</td>\n",
       "      <td>0.511779</td>\n",
       "      <td>0.580696</td>\n",
       "      <td>0.507602</td>\n",
       "      <td>0.476703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>uruk_in_pess</td>\n",
       "      <td>0.232589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022487</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.726352</td>\n",
       "      <td>0.265441</td>\n",
       "      <td>0.026098</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349410</td>\n",
       "      <td>0.431437</td>\n",
       "      <td>0.382375</td>\n",
       "      <td>0.743416</td>\n",
       "      <td>0.420450</td>\n",
       "      <td>0.532757</td>\n",
       "      <td>0.632054</td>\n",
       "      <td>0.555310</td>\n",
       "      <td>0.584002</td>\n",
       "      <td>0.454950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>ylf_alph_mindon</td>\n",
       "      <td>0.112045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042328</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.483645</td>\n",
       "      <td>0.252147</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>0.689171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322501</td>\n",
       "      <td>0.441712</td>\n",
       "      <td>0.706573</td>\n",
       "      <td>0.568809</td>\n",
       "      <td>0.502470</td>\n",
       "      <td>0.483427</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.695547</td>\n",
       "      <td>0.461846</td>\n",
       "      <td>0.486949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2876 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     object_id         Z  Z_err       EBV  target   u_n_obs  \\\n",
       "0     Dornhoth_fervain_onodrim  0.618533    0.0  0.142857       0  0.012658   \n",
       "1          Dornhoth_galadh_ylf  0.086187    0.0  0.074074       0  0.075949   \n",
       "3           Ithil_tobas_rodwen  0.139531    0.0  0.013228       0  0.664557   \n",
       "4          Mirion_adar_Druadan  0.082871    0.0  0.074074       0  0.063291   \n",
       "5        Mirion_lalaith_neledh  0.241744    0.0  0.068783       0  0.031646   \n",
       "...                        ...       ...    ...       ...     ...       ...   \n",
       "3037       tinnu_gellui_tathar  0.179245    0.0  0.052910       0  0.082278   \n",
       "3038            uir_heleg_corf  0.193486    0.0  0.052910       0  0.063291   \n",
       "3039             uir_rhosc_law  0.029608    0.0  0.029101       0  0.082278   \n",
       "3040              uruk_in_pess  0.232589    0.0  0.022487       0  0.063291   \n",
       "3041           ylf_alph_mindon  0.112045    0.0  0.042328       0  0.088608   \n",
       "\n",
       "      u_time_span  u_flux_mean  u_flux_std  u_flux_min  ...  color_mean_g_i  \\\n",
       "0        0.234777     0.263663    0.025473    0.701691  ...        0.220216   \n",
       "1        0.634268     0.255375    0.013209    0.684591  ...        0.325172   \n",
       "3        0.809191     0.257249    0.010491    0.670563  ...        0.325453   \n",
       "4        0.523706     0.254764    0.009648    0.683958  ...        0.317713   \n",
       "5        0.515954     0.261257    0.006607    0.701172  ...        0.315120   \n",
       "...           ...          ...         ...         ...  ...             ...   \n",
       "3037     0.617956     0.253951    0.009370    0.684037  ...        0.318977   \n",
       "3038     0.817602     0.271857    0.051119    0.640874  ...        0.348592   \n",
       "3039     0.514983     0.255588    0.011571    0.680742  ...        0.316626   \n",
       "3040     0.726352     0.265441    0.026098    0.658400  ...        0.349410   \n",
       "3041     0.483645     0.252147    0.005334    0.689171  ...        0.322501   \n",
       "\n",
       "      color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
       "0           0.430231         0.461601         0.625388         0.489657   \n",
       "1           0.426179         0.378973         0.626645         0.494262   \n",
       "3           0.437208         0.846568         0.389680         0.759607   \n",
       "4           0.437885         0.524558         0.577734         0.477055   \n",
       "5           0.412191         0.515052         0.820523         0.267317   \n",
       "...              ...              ...              ...              ...   \n",
       "3037        0.414826         0.926462         0.515835         0.553827   \n",
       "3038        0.449292         0.533710         0.615690         0.503236   \n",
       "3039        0.408622         0.545142         0.617131         0.491311   \n",
       "3040        0.431437         0.382375         0.743416         0.420450   \n",
       "3041        0.441712         0.706573         0.568809         0.502470   \n",
       "\n",
       "      t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
       "0            0.481269         0.508584         0.501736         0.515684   \n",
       "1            0.471119         0.507978         0.415584         0.521944   \n",
       "3            0.489029         0.175839         0.635187         0.515684   \n",
       "4            0.578441         0.404345         0.512945         0.446111   \n",
       "5            0.756529         0.218031         0.785879         0.516868   \n",
       "...               ...              ...              ...              ...   \n",
       "3037         0.444562         0.152402         0.866962         0.452201   \n",
       "3038         0.519005         0.452515         0.566894         0.518243   \n",
       "3039         0.483898         0.511779         0.580696         0.507602   \n",
       "3040         0.532757         0.632054         0.555310         0.584002   \n",
       "3041         0.483427         0.506849         0.695547         0.461846   \n",
       "\n",
       "      t_peak_diff_r_z  \n",
       "0            0.472637  \n",
       "1            0.467464  \n",
       "3            0.738604  \n",
       "4            0.552323  \n",
       "5            0.519544  \n",
       "...               ...  \n",
       "3037         0.499455  \n",
       "3038         0.521281  \n",
       "3039         0.476703  \n",
       "3040         0.454950  \n",
       "3041         0.486949  \n",
       "\n",
       "[2876 rows x 117 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3217d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING y_base\n",
      "============================================================\n",
      "y_base type: <class 'pandas.core.series.Series'>\n",
      "y_base shape: (2876,)\n",
      "y_base ndim: 1\n",
      "First 10 values:\n",
      "0     0\n",
      "1     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "Name: target, dtype: int64\n",
      "Value counts:\n",
      "target\n",
      "0    2729\n",
      "1     147\n",
      "Name: count, dtype: int64\n",
      "\n",
      "X_base_features shape: (2876, 114)\n"
     ]
    }
   ],
   "source": [
    "X_base = df_scaled.copy()\n",
    "X_base.drop(['Z_err'], axis=1, inplace=True)\n",
    "y_base = X_base['target'].copy()\n",
    "\n",
    "# KIỂM TRA y_base\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKING y_base\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"y_base type: {type(y_base)}\")\n",
    "print(f\"y_base shape: {y_base.shape}\")\n",
    "print(f\"y_base ndim: {y_base.ndim}\")\n",
    "print(f\"First 10 values:\\n{y_base.head(10)}\")\n",
    "print(f\"Value counts:\\n{y_base.value_counts()}\")\n",
    "\n",
    "exclude_cols = ['object_id', 'Z_err', 'target']\n",
    "feature_cols_base = [col for col in X_base.columns \n",
    "                     if col not in exclude_cols \n",
    "                     and X_base[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "X_base_features = X_base[feature_cols_base].copy()\n",
    "print(f\"\\nX_base_features shape: {X_base_features.shape}\")\n",
    "# X_base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f05ad8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số hàng có NaN: 0\n",
      "\n",
      "Các hàng có NaN:\n",
      "Empty DataFrame\n",
      "Columns: [object_id, Z, Z_err, EBV, target, u_n_obs, u_time_span, u_flux_mean, u_flux_std, u_flux_min, u_flux_max, u_flux_median, u_amplitude, u_flux_skew, u_flux_kurtosis, u_t_peak, u_rise_time, u_decay_time, u_max_slope_up, u_max_slope_down, u_auc, g_n_obs, g_time_span, g_flux_mean, g_flux_std, g_flux_min, g_flux_max, g_flux_median, g_amplitude, g_flux_skew, g_flux_kurtosis, g_t_peak, g_rise_time, g_decay_time, g_max_slope_up, g_max_slope_down, g_auc, r_n_obs, r_time_span, r_flux_mean, r_flux_std, r_flux_min, r_flux_max, r_flux_median, r_amplitude, r_flux_skew, r_flux_kurtosis, r_t_peak, r_rise_time, r_decay_time, r_max_slope_up, r_max_slope_down, r_auc, i_n_obs, i_time_span, i_flux_mean, i_flux_std, i_flux_min, i_flux_max, i_flux_median, i_amplitude, i_flux_skew, i_flux_kurtosis, i_t_peak, i_rise_time, i_decay_time, i_max_slope_up, i_max_slope_down, i_auc, z_n_obs, z_time_span, z_flux_mean, z_flux_std, z_flux_min, z_flux_max, z_flux_median, z_amplitude, z_flux_skew, z_flux_kurtosis, z_t_peak, z_rise_time, z_decay_time, z_max_slope_up, z_max_slope_down, z_auc, y_n_obs, y_time_span, y_flux_mean, y_flux_std, y_flux_min, y_flux_max, y_flux_median, y_amplitude, y_flux_skew, y_flux_kurtosis, y_t_peak, y_rise_time, y_decay_time, y_max_slope_up, y_max_slope_down, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 117 columns]\n",
      "\n",
      "Index của các hàng có NaN: []\n"
     ]
    }
   ],
   "source": [
    "rows_with_nan = X_base.isnull().any(axis=1)\n",
    "print(\"Số hàng có NaN:\", rows_with_nan.sum())\n",
    "\n",
    "# Lấy ra các hàng có NaN\n",
    "df_with_nan = df[rows_with_nan]\n",
    "print(\"\\nCác hàng có NaN:\")\n",
    "print(df_with_nan)\n",
    "\n",
    "# Lấy index của các hàng có NaN\n",
    "nan_indices = df[rows_with_nan].index.tolist()\n",
    "print(\"\\nIndex của các hàng có NaN:\", nan_indices[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30df916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix,\n",
    "                             classification_report, roc_curve)\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import models\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              BaggingClassifier, AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e2f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    num_classes = cm.shape[0]\n",
    "    specificity_list = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # TN = tổng tất cả phần tử ngoại trừ hàng i và cột i\n",
    "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
    "        # FP = tổng cột i (ngoại trừ phần tử đúng)\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "        specificity_list.append(specificity)\n",
    "\n",
    "    # trung bình macro (mỗi lớp có trọng số bằng nhau)\n",
    "    return np.mean(specificity_list)\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    specificity = calculate_specificity(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        # Binary classification: chỉ lấy xác suất của class dương (class 1)\n",
    "        if y_pred_proba.shape[1] == 2:\n",
    "            auc_roc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            auc_roc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    else:\n",
    "        auc_roc = np.nan\n",
    "        \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Specificity': specificity,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1 Score': f1,\n",
    "        'AUC': auc_roc\n",
    "    }\n",
    "    return results, y_pred, y_pred_proba\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=5, use_stratified=True):\n",
    "    \"\"\"\n",
    "    Thực hiện k-fold cross-validation với đầy đủ metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Model cần đánh giá\n",
    "    X : array-like\n",
    "        Features\n",
    "    y : array-like\n",
    "        Labels\n",
    "    cv : int\n",
    "        Số lượng folds (mặc định 5)\n",
    "    use_stratified : bool\n",
    "        Sử dụng StratifiedKFold để đảm bảo phân phối lớp (mặc định True)\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    if use_stratified:\n",
    "        kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Tính toán các metrics cho từng fold\n",
    "    fold_accuracies = []\n",
    "    fold_recalls = []\n",
    "    fold_precisions = []\n",
    "    fold_f1s = []\n",
    "    fold_specificities = []\n",
    "    fold_aucs = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model_copy = clone(model)\n",
    "        model_copy.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_fold = model_copy.predict(X_val_fold)\n",
    "        y_pred_proba_fold = model_copy.predict_proba(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fold_accuracies.append(accuracy_score(y_val_fold, y_pred_fold))\n",
    "        fold_recalls.append(recall_score(y_val_fold, y_pred_fold, average='macro', zero_division=0))\n",
    "        fold_precisions.append(precision_score(y_val_fold, y_pred_fold, average='macro', zero_division=0))\n",
    "        fold_f1s.append(f1_score(y_val_fold, y_pred_fold, average='macro', zero_division=0))\n",
    "        fold_specificities.append(calculate_specificity(y_val_fold, y_pred_fold))\n",
    "        \n",
    "        try:\n",
    "            # Binary classification: chỉ lấy xác suất của class dương (class 1)\n",
    "            if y_pred_proba_fold.shape[1] == 2:\n",
    "                fold_aucs.append(roc_auc_score(y_val_fold, y_pred_proba_fold[:, 1]))\n",
    "            else:\n",
    "                # Multi-class classification\n",
    "                fold_aucs.append(roc_auc_score(y_val_fold, y_pred_proba_fold, multi_class='ovr', average='macro'))\n",
    "        except:\n",
    "            fold_aucs.append(np.nan)\n",
    "    \n",
    "    # Tính mean và std cho tất cả metrics\n",
    "    cv_results = {\n",
    "        'CV_Accuracy_mean': np.mean(fold_accuracies),\n",
    "        'CV_Accuracy_std': np.std(fold_accuracies),\n",
    "        'CV_Recall_mean': np.mean(fold_recalls),\n",
    "        'CV_Recall_std': np.std(fold_recalls),\n",
    "        'CV_Precision_mean': np.mean(fold_precisions),\n",
    "        'CV_Precision_std': np.std(fold_precisions),\n",
    "        'CV_F1_mean': np.mean(fold_f1s),\n",
    "        'CV_F1_std': np.std(fold_f1s),\n",
    "        'CV_Specificity_mean': np.mean(fold_specificities),\n",
    "        'CV_Specificity_std': np.std(fold_specificities),\n",
    "        'CV_AUC_mean': np.nanmean(fold_aucs),\n",
    "        'CV_AUC_std': np.nanstd(fold_aucs),\n",
    "        'CV_Fold_Details': {\n",
    "            'accuracies': fold_accuracies,\n",
    "            'recalls': fold_recalls,\n",
    "            'precisions': fold_precisions,\n",
    "            'f1s': fold_f1s,\n",
    "            'specificities': fold_specificities,\n",
    "            'aucs': fold_aucs\n",
    "        }\n",
    "    }\n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796795a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Train-Test Split (80:20)\n",
      "============================================================\n",
      "y_base type: <class 'pandas.core.series.Series'>\n",
      "y_base shape: (2876,)\n",
      "y_base dtype: int64\n",
      "y_base first 5 values:\n",
      "0    0\n",
      "1    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "Name: target, dtype: int64\n",
      "\n",
      "y_train type: <class 'pandas.core.series.Series'>\n",
      "y_train shape: (2300,)\n",
      "y_test type: <class 'pandas.core.series.Series'>\n",
      "y_test shape: (576,)\n",
      "\n",
      "Train set shape: (2300, 114)\n",
      "Test set shape: (576, 114)\n",
      "Train set size: 2300 samples (80.0%)\n",
      "Test set size: 576 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Train-Test Split (80:20)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# KIỂM TRA y_base trước khi split\n",
    "print(f\"y_base type: {type(y_base)}\")\n",
    "print(f\"y_base shape: {y_base.shape}\")\n",
    "print(f\"y_base dtype: {y_base.dtype}\")\n",
    "print(f\"y_base first 5 values:\\n{y_base.head()}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_base_features, y_base, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_base\n",
    ")\n",
    "\n",
    "# KIỂM TRA sau khi split\n",
    "print(f\"\\ny_train type: {type(y_train)}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test type: {type(y_test)}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTrain set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Train set size: {len(X_train)} samples ({len(X_train)/len(X_base_features)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(X_base_features)*100:.1f}%)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9e67e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEFINING MODELS WITH HYPERPARAMETERS\n",
      "================================================================================\n",
      "Total models to evaluate: 9\n",
      "  - RF\n",
      "  - LR\n",
      "  - ANN\n",
      "  - ETC\n",
      "  - Bagging\n",
      "  - XGB\n",
      "  - AdaBoost\n",
      "  - CatBoost\n",
      "  - LightGBM\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEFINING MODELS WITH HYPERPARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "models = {\n",
    "    'RF': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        criterion='gini',\n",
    "        max_depth=None,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'LR': LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'ANN': MLPClassifier(\n",
    "        hidden_layer_sizes=(100,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'ETC': ExtraTreesClassifier(\n",
    "        n_estimators=100,\n",
    "        criterion='gini',\n",
    "        max_depth=None,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'Bagging': BaggingClassifier(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        n_estimators=10,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'XGB': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        n_estimators=50,\n",
    "        learning_rate=1.0,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Total models to evaluate: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283a0c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING AND EVALUATING MODELS\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training RF...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for RF...\n",
      "Test Accuracy: 0.9497\n",
      "Test AUC: 0.8544\n",
      "CV Accuracy: 0.9496 (+/- 0.0016)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training LR...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for LR...\n",
      "Test Accuracy: 0.9497\n",
      "Test AUC: 0.7388\n",
      "CV Accuracy: 0.9487 (+/- 0.0011)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training ANN...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for ANN...\n",
      "Test Accuracy: 0.9462\n",
      "Test AUC: 0.8919\n",
      "CV Accuracy: 0.9487 (+/- 0.0033)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training ETC...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for ETC...\n",
      "Test Accuracy: 0.9497\n",
      "Test AUC: 0.8593\n",
      "CV Accuracy: 0.9487 (+/- 0.0011)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training Bagging...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for Bagging...\n",
      "Test Accuracy: 0.9497\n",
      "Test AUC: 0.8657\n",
      "CV Accuracy: 0.9487 (+/- 0.0011)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training XGB...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for XGB...\n",
      "Test Accuracy: 0.9514\n",
      "Test AUC: 0.8843\n",
      "CV Accuracy: 0.9496 (+/- 0.0042)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training AdaBoost...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for AdaBoost...\n",
      "Test Accuracy: 0.9497\n",
      "Test AUC: 0.8164\n",
      "CV Accuracy: 0.9461 (+/- 0.0025)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training CatBoost...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for CatBoost...\n",
      "Test Accuracy: 0.9514\n",
      "Test AUC: 0.8940\n",
      "CV Accuracy: 0.9509 (+/- 0.0033)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training LightGBM...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for LightGBM...\n",
      "Test Accuracy: 0.9497\n",
      "Test AUC: 0.9020\n",
      "CV Accuracy: 0.9522 (+/- 0.0041)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING AND EVALUATING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = []\n",
    "detailed_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    results, y_pred, y_pred_proba = evaluate_model(\n",
    "        model, X_train, X_test, y_train, y_test, model_name\n",
    "    )\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    print(f\"Performing 5-Fold Cross-Validation for {model_name}...\")\n",
    "    cv_results = cross_validate_model(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Combine results\n",
    "    results.update(cv_results)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Store detailed results\n",
    "    detailed_results[model_name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Test Accuracy: {results['Accuracy']:.4f}\")\n",
    "    print(f\"Test AUC: {results['AUC']:.4f}\")\n",
    "    print(f\"CV Accuracy: {results['CV_Accuracy_mean']:.4f} (+/- {results['CV_Accuracy_std']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e8f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32265662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3451b596",
   "metadata": {},
   "source": [
    "# 📊 INFERENCE & PREDICTION\n",
    "\n",
    "Phần này sẽ:\n",
    "1. Load dữ liệu test\n",
    "2. Preprocessing (giống training)\n",
    "3. Dự đoán với model tốt nhất\n",
    "4. Tạo file CSV submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5150e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING TEST DATA\n",
      "============================================================\n",
      "Test data shape: (7135, 116)\n",
      "\n",
      "First 3 rows:\n",
      "                  object_id       Z    Z_err    EBV  u_n_obs  u_time_span  \\\n",
      "0  Eluwaith_Mithrim_nothrim  0.5393  0.03013  0.610        2    1216.0901   \n",
      "1        Eru_heledir_archam  0.7282  0.03508  0.058        9    1830.1833   \n",
      "2         Gonhir_anann_fuin  0.6026  0.03185  0.070       17    2359.4595   \n",
      "\n",
      "   u_flux_mean  u_flux_std  u_flux_min  u_flux_max  ...  color_mean_g_i  \\\n",
      "0     0.080995    0.063499    0.017497    0.144494  ...       -2.848042   \n",
      "1     0.318856    0.438312   -0.472787    1.009501  ...       -0.941344   \n",
      "2     0.154386    0.809561   -1.896650    1.650006  ...       -0.181608   \n",
      "\n",
      "   color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
      "0       -2.747490         -28.9546        -636.9996         242.7726   \n",
      "1       -0.350325         411.1488         -11.4208         702.3793   \n",
      "2        0.047289        -212.9875        -652.7930         -38.7250   \n",
      "\n",
      "   t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
      "0           6.6818              NaN        -665.9542        -394.2270   \n",
      "1          48.5384        -119.9184         399.7280         690.9585   \n",
      "2        1385.8021       -2104.9809        -865.7805        -691.5180   \n",
      "\n",
      "   t_peak_diff_r_z  \n",
      "0         249.4544  \n",
      "1         750.9177  \n",
      "2        1347.0771  \n",
      "\n",
      "[3 rows x 116 columns]\n",
      "\n",
      "Columns: ['object_id', 'Z', 'Z_err', 'EBV', 'u_n_obs', 'u_time_span', 'u_flux_mean', 'u_flux_std', 'u_flux_min', 'u_flux_max']...\n",
      "\n",
      "Checking for NaN:\n",
      "Total NaN: 8881\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD TEST DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv('/home/duy/Downloads/Mallorn/Mallorn/mallorn-astronomical-classification-challenge/test_features_ml.csv')\n",
    "\n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_test.head(3))\n",
    "print(f\"\\nColumns: {df_test.columns.tolist()[:10]}...\")\n",
    "print(f\"\\nChecking for NaN:\")\n",
    "print(f\"Total NaN: {df_test.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9d05d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPROCESSING TEST DATA\n",
      "============================================================\n",
      "\n",
      "2. Điền MEDIAN vào các cột số còn lại...\n",
      "   ✓ Đã điền MEDIAN vào 91 cột\n",
      "\n",
      "3. Saved 7135 object IDs for submission\n",
      "\n",
      "4. Scaling features...\n",
      "   Columns to scale: 114\n",
      "   ✓ Scaled using training scaler\n",
      "\n",
      "5. Preparing features...\n",
      "   ✓ Test features shape: (7135, 114)\n",
      "\n",
      "============================================================\n",
      "✓ TEST DATA READY FOR INFERENCE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PREPROCESS TEST DATA (SAME AS TRAINING)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# # 1. Fill Z_err with 0\n",
    "# print(\"1. Filling Z_err with 0...\")\n",
    "# if 'Z_err' in df_test.columns:\n",
    "#     df_test['Z_err'] = df_test['Z_err'].fillna(0)\n",
    "#     print(f\"   ✓ Filled Z_err\")\n",
    "\n",
    "# 2. Drop rows with NaN in other columns\n",
    "# print(\"\\n2. Dropping rows with NaN in other columns...\")\n",
    "# rows_before = len(df_test)\n",
    "# df_test_cleaned = df_test.dropna()\n",
    "# rows_after = len(df_test_cleaned)\n",
    "# print(f\"   ✓ Dropped {rows_before - rows_after} rows ({(rows_before - rows_after)/rows_before*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# 2. Điền MEDIAN vào các cột số còn lại\n",
    "print(\"\\n2. Điền MEDIAN vào các cột số còn lại...\")\n",
    "numerical_cols = df_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['Z_err', 'target']]\n",
    "\n",
    "filled_count = 0\n",
    "for col in numerical_cols:\n",
    "    if df_test[col].isnull().sum() > 0:\n",
    "        median_value = df_test[col].median()\n",
    "        df_test[col] = df_test[col].fillna(median_value)\n",
    "        filled_count += 1\n",
    "\n",
    "print(f\"   ✓ Đã điền MEDIAN vào {filled_count} cột\")\n",
    "\n",
    "# 3. Save object_id for submission\n",
    "test_object_ids = df_test['object_id'].copy()\n",
    "print(f\"\\n3. Saved {len(test_object_ids)} object IDs for submission\")\n",
    "\n",
    "# 4. Scale the features (using the SAME scaler from training)\n",
    "print(\"\\n4. Scaling features...\")\n",
    "df_test_scaled = df_test.copy()\n",
    "\n",
    "# Get columns to scale (same as training)\n",
    "columns_to_scale_test = [col for col in df_test_scaled.columns \n",
    "                          if col not in ['object_id', 'Z_err'] and col in columns_to_scale]\n",
    "\n",
    "print(f\"   Columns to scale: {len(columns_to_scale_test)}\")\n",
    "\n",
    "# Apply the SAME scaler from training data\n",
    "df_test_scaled[columns_to_scale_test] = scaler.transform(df_test_scaled[columns_to_scale_test])\n",
    "print(f\"   ✓ Scaled using training scaler\")\n",
    "\n",
    "# 5. Prepare features (same as training)\n",
    "print(\"\\n5. Preparing features...\")\n",
    "exclude_cols_test = ['object_id', 'Z_err']\n",
    "feature_cols_test = [col for col in df_test_scaled.columns \n",
    "                     if col not in exclude_cols_test \n",
    "                     and df_test_scaled[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "X_test_features = df_test_scaled[feature_cols_test].copy()\n",
    "\n",
    "print(f\"   ✓ Test features shape: {X_test_features.shape}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TEST DATA READY FOR INFERENCE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# df_test = df_test.copy()\n",
    "\n",
    "# # 1. Điền 0 vào cột Z_err\n",
    "# print(\"1. Điền giá trị 0 vào cột Z_err...\")\n",
    "# df_test['Z_err'] = df_test['Z_err'].fillna(0)\n",
    "# print(f\"   ✓ Đã điền 0 vào {nan_counts['Z_err']} giá trị NaN trong Z_err\")\n",
    "\n",
    "# # 2. Điền MEDIAN vào các cột số còn lại\n",
    "# print(\"\\n2. Điền MEDIAN vào các cột số còn lại...\")\n",
    "# numerical_cols = df_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "# numerical_cols = [col for col in numerical_cols if col not in ['Z_err', 'target']]\n",
    "\n",
    "# filled_count = 0\n",
    "# for col in numerical_cols:\n",
    "#     if df_test[col].isnull().sum() > 0:\n",
    "#         median_value = df_test[col].median()\n",
    "#         df_test[col] = df_test[col].fillna(median_value)\n",
    "#         filled_count += 1\n",
    "\n",
    "# print(f\"   ✓ Đã điền MEDIAN vào {filled_count} cột\")\n",
    "\n",
    "# # 3. Kiểm tra xem còn NaN không\n",
    "# print(\"\\n3. Kiểm tra lại...\")\n",
    "# remaining_nan = df_test.isnull().sum().sum()\n",
    "# if remaining_nan > 0:\n",
    "#     print(f\"   ⚠️ Còn {remaining_nan} NaN (có thể ở cột text)\")\n",
    "#     object_cols = df_test.select_dtypes(include=['object']).columns\n",
    "#     for col in object_cols:\n",
    "#         if col != 'object_id':\n",
    "#             df_test[col] = df_test[col].fillna('Unknown')\n",
    "#     print(\"   ✓ Đã điền 'Unknown' vào các cột text\")\n",
    "# else:\n",
    "#     print(\"   ✓ Không còn NaN\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"SAU KHI XỬ LÝ NaN\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"Kích thước DataFrame: {df_test.shape} (KHÔNG XÓA HÀNG ✓)\")\n",
    "# print(\"Số hàng đã xóa: 0\")\n",
    "# print(f\"\\nCòn NaN trong DataFrame không? {df_test.isnull().any().any()}\")\n",
    "# print(f\"Tổng số NaN còn lại: {df_test.isnull().sum().sum()}\")\n",
    "\n",
    "# # Hiển thị thống kê cột Z_err sau khi điền\n",
    "# print(\"\\nThống kê cột Z_err sau khi điền 0:\")\n",
    "# print(f\"  - Min: {df_test['Z_err'].min()}\")\n",
    "# print(f\"  - Max: {df_test['Z_err'].max()}\")\n",
    "# print(f\"  - Mean: {df_test['Z_err'].mean():.4f}\")\n",
    "# print(f\"  - Số giá trị = 0: {(df_test['Z_err'] == 0).sum()}\")\n",
    "\n",
    "# # Cập nhật df với bản đã điền\n",
    "# df = df_test.copy()\n",
    "\n",
    "# print(\"\\n✓ DataFrame đã được cập nhật!\")\n",
    "# print(f\"✓ Kích thước hiện tại: {df.shape}\")\n",
    "# print(f\"✓ Đã GIỮ NGUYÊN tất cả {len(df)} hàng dữ liệu!\")\n",
    "\n",
    "# X_test_features = df_test_scaled[feature_cols_test].copy()\n",
    "\n",
    "# print(f\"   ✓ Test features shape: {X_test_features.shape}\")\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"✓ TEST DATA READY FOR INFERENCE!\")\n",
    "# print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c3dbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MAKING PREDICTIONS WITH BEST MODELS + THRESHOLD\n",
      "============================================================\n",
      "\n",
      "Top 3 Models by CV Accuracy:\n",
      "      Model  CV_Accuracy_mean  CV_AUC_mean       AUC\n",
      "8  LightGBM          0.952174     0.902331  0.901973\n",
      "7  CatBoost          0.950870     0.919461  0.893967\n",
      "0        RF          0.949565     0.868591  0.854410\n",
      "\n",
      "🏆 Best Model: LightGBM\n",
      "   CV Accuracy: 0.9522\n",
      "   CV AUC: 0.9023\n",
      "\n",
      " Using manual threshold = 0.05\n",
      "\n",
      "Making predictions with LightGBM...\n",
      "✓ Predictions completed!\n",
      "\n",
      "Prediction distribution:\n",
      "Class 0: 6859 (96.13%)\n",
      "Class 1: 276 (3.87%)\n",
      "\n",
      "Probability stats (class 1):\n",
      "min = 0.000002, mean = 0.015040, max = 0.997878\n",
      "\n",
      "✓ Submission file created: /home/duy/Downloads/Mallorn_update/Mallorn/submission.csv\n",
      "prediction\n",
      "0    6859\n",
      "1     276\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ============================================================\n",
    "# MAKE PREDICTIONS WITH BEST MODELS + CUSTOM THRESHOLD\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MAKING PREDICTIONS WITH BEST MODELS + THRESHOLD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# 1) LẤY TOP MODEL THEO CV ACCURACY\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df_sorted = results_df.sort_values(\n",
    "    'CV_Accuracy_mean', ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\nTop 3 Models by CV Accuracy:\")\n",
    "print(\n",
    "    results_df_sorted[\n",
    "        ['Model', 'CV_Accuracy_mean', 'CV_AUC_mean', 'AUC']\n",
    "    ].head(3)\n",
    ")\n",
    "\n",
    "best_model_name = results_df_sorted.iloc[0]['Model']\n",
    "best_model = detailed_results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"   CV Accuracy: {results_df_sorted.iloc[0]['CV_Accuracy_mean']:.4f}\")\n",
    "print(f\"   CV AUC: {results_df_sorted.iloc[0]['CV_AUC_mean']:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) CHỌN THRESHOLD\n",
    "# ============================================================\n",
    "def best_f1_threshold(y_true, proba1):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, proba1)\n",
    "    f1 = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1))\n",
    "    return float(thresholds[best_idx]), float(f1[best_idx])\n",
    "\n",
    "USE_VALIDATION = False   # đổi True nếu bạn có X_val_features, y_val\n",
    "\n",
    "if USE_VALIDATION:\n",
    "    #  dùng X_test / y_test của bạn rồi comment lại đoạn này nếu cần\n",
    "    proba_val = best_model.predict_proba(X_test_features)[:, 1]\n",
    "    thr, best_f1 = best_f1_threshold(y_test.values, proba_val)\n",
    "\n",
    "    print(f\"\\n✓ Best threshold from validation = {thr:.4f}\")\n",
    "    print(f\"✓ Best F1 (val) = {best_f1:.4f}\")\n",
    "else:\n",
    "    thr = 0.05   # thử: 0.01 / 0.02 / 0.05 / 0.1\n",
    "    print(f\"\\n Using manual threshold = {thr}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) PREDICT PROBA + ÁP THRESHOLD\n",
    "# ============================================================\n",
    "print(f\"\\nMaking predictions with {best_model_name}...\")\n",
    "\n",
    "# ❗ đoạn bạn nói: dùng X_test_features, cần thì comment lại\n",
    "proba_test = best_model.predict_proba(X_test_features)[:, 1]\n",
    "y_pred_test = (proba_test >= thr).astype(int)\n",
    "\n",
    "print(\"✓ Predictions completed!\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) THỐNG KÊ KẾT QUẢ\n",
    "# ============================================================\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(f\"Class 0: {(y_pred_test == 0).sum()} ({(y_pred_test == 0).mean()*100:.2f}%)\")\n",
    "print(f\"Class 1: {(y_pred_test == 1).sum()} ({(y_pred_test == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nProbability stats (class 1):\")\n",
    "print(\n",
    "    f\"min = {proba_test.min():.6f}, \"\n",
    "    f\"mean = {proba_test.mean():.6f}, \"\n",
    "    f\"max = {proba_test.max():.6f}\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5) TẠO SUBMISSION\n",
    "# ============================================================\n",
    "submission = pd.DataFrame({\n",
    "    \"object_id\": test_object_ids,\n",
    "    \"prediction\": y_pred_test\n",
    "})\n",
    "\n",
    "submission_path = \"/home/duy/Downloads/Mallorn_update/Mallorn/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Submission file created: {submission_path}\")\n",
    "print(submission[\"prediction\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0731ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENSEMBLE PREDICTIONS WITH TOP 3 MODELS (SOFT VOTING)\n",
      "============================================================\n",
      "Top 3 models: ['LightGBM', 'CatBoost', 'RF']\n",
      "  LightGBM: proba mean = 0.015040, class1@thr = 276 (3.87%)\n",
      "  CatBoost: proba mean = 0.020246, class1@thr = 514 (7.20%)\n",
      "  RF: proba mean = 0.058995, class1@thr = 3082 (43.20%)\n",
      "\n",
      "✓ Ensemble prediction distribution:\n",
      "Class 0: 6182 (86.64%)\n",
      "Class 1: 953 (13.36%)\n",
      "\n",
      "Ensemble proba stats: min=0.000005 mean=0.031427 max=0.828167\n",
      "\n",
      "✓ Ensemble submission saved: /home/duy/Downloads/Mallorn/Mallorn/submission_ensemble.csv\n",
      "prediction\n",
      "0    6182\n",
      "1     953\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 10 rows:\n",
      "                      object_id  prediction\n",
      "0      Eluwaith_Mithrim_nothrim           0\n",
      "1            Eru_heledir_archam           0\n",
      "2             Gonhir_anann_fuin           0\n",
      "3  Gwathuirim_haradrim_tegilbor           0\n",
      "4              achas_minai_maen           0\n",
      "5                 adab_fae_gath           0\n",
      "6               adel_draug_gaur           0\n",
      "7       aderthad_cuil_galadhrim           0\n",
      "8           aegas_laug_ithildin           0\n",
      "9              aegas_mereth_law           0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ENSEMBLE PREDICTIONS (SOFT VOTING)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENSEMBLE PREDICTIONS WITH TOP 3 MODELS (SOFT VOTING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# dùng cùng threshold đã chọn ở cell trước\n",
    "# thr = 0.02\n",
    "\n",
    "# lấy top 3 models\n",
    "top_3_models = results_df_sorted.head(3)[\"Model\"].tolist()\n",
    "print(f\"Top 3 models: {top_3_models}\")\n",
    "\n",
    "proba_list = []\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    model = detailed_results[model_name][\"model\"]\n",
    "\n",
    "    # dùng trực tiếp X_test_features\n",
    "    proba = model.predict_proba(X_test_features)[:, 1]\n",
    "    proba_list.append(proba)\n",
    "\n",
    "    print(\n",
    "        f\"  {model_name}: \"\n",
    "        f\"proba mean = {proba.mean():.6f}, \"\n",
    "        f\"class1@thr = {(proba >= thr).sum()} \"\n",
    "        f\"({(proba >= thr).mean()*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "# trung bình xác suất\n",
    "proba_ensemble = np.mean(proba_list, axis=0)\n",
    "\n",
    "# áp threshold\n",
    "y_pred_ensemble = (proba_ensemble >= thr).astype(int)\n",
    "\n",
    "print(\"\\n✓ Ensemble prediction distribution:\")\n",
    "print(f\"Class 0: {(y_pred_ensemble == 0).sum()} ({(y_pred_ensemble == 0).mean()*100:.2f}%)\")\n",
    "print(f\"Class 1: {(y_pred_ensemble == 1).sum()} ({(y_pred_ensemble == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "print(\n",
    "    \"\\nEnsemble proba stats:\",\n",
    "    f\"min={proba_ensemble.min():.6f}\",\n",
    "    f\"mean={proba_ensemble.mean():.6f}\",\n",
    "    f\"max={proba_ensemble.max():.6f}\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE ENSEMBLE SUBMISSION\n",
    "# ============================================================\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    \"object_id\": test_object_ids,\n",
    "    \"prediction\": y_pred_ensemble\n",
    "})\n",
    "\n",
    "ensemble_path = \"/home/duy/Downloads/Mallorn/Mallorn/submission_ensemble.csv\"\n",
    "submission_ensemble.to_csv(ensemble_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Ensemble submission saved: {ensemble_path}\")\n",
    "print(submission_ensemble[\"prediction\"].value_counts())\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(submission_ensemble.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fcf82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENSEMBLE PREDICTIONS (LGBM + CATBOOST)\n",
      "============================================================\n",
      "Ensemble models: ['LightGBM', 'CatBoost']\n",
      "  LightGBM: proba mean = 0.015040, class1@thr = 276 (3.87%)\n",
      "  CatBoost: proba mean = 0.020246, class1@thr = 514 (7.20%)\n",
      "\n",
      "✓ Ensemble prediction distribution:\n",
      "Class 0: 6738 (94.44%)\n",
      "Class 1: 397 (5.56%)\n",
      "\n",
      "Ensemble proba stats: min=0.000008 mean=0.017643 max=0.962250\n",
      "\n",
      "✓ Ensemble submission saved: /home/duy/Downloads/Mallorn_update/Mallorn/submission_ensemble.csv\n",
      "prediction\n",
      "0    6738\n",
      "1     397\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 10 rows:\n",
      "                      object_id  prediction\n",
      "0      Eluwaith_Mithrim_nothrim           0\n",
      "1            Eru_heledir_archam           0\n",
      "2             Gonhir_anann_fuin           0\n",
      "3  Gwathuirim_haradrim_tegilbor           0\n",
      "4              achas_minai_maen           0\n",
      "5                 adab_fae_gath           0\n",
      "6               adel_draug_gaur           0\n",
      "7       aderthad_cuil_galadhrim           0\n",
      "8           aegas_laug_ithildin           0\n",
      "9              aegas_mereth_law           0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ENSEMBLE PREDICTIONS (BEST PRACTICE FOR MALLORN)\n",
    "# LightGBM + CatBoost (NO RANDOM FOREST)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENSEMBLE PREDICTIONS (LGBM + CATBOOST)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# threshold đã chọn ở cell trước\n",
    "# thr = 0.02\n",
    "\n",
    "# chọn 2 model ổn định nhất\n",
    "ensemble_models = [\"LightGBM\", \"CatBoost\"]\n",
    "print(f\"Ensemble models: {ensemble_models}\")\n",
    "\n",
    "proba_list = []\n",
    "\n",
    "for model_name in ensemble_models:\n",
    "    model = detailed_results[model_name][\"model\"]\n",
    "    proba = model.predict_proba(X_test_features)[:, 1]\n",
    "    proba_list.append(proba)\n",
    "\n",
    "    print(\n",
    "        f\"  {model_name}: \"\n",
    "        f\"proba mean = {proba.mean():.6f}, \"\n",
    "        f\"class1@thr = {(proba >= thr).sum()} \"\n",
    "        f\"({(proba >= thr).mean()*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "# soft voting = trung bình xác suất\n",
    "proba_ensemble = np.mean(proba_list, axis=0)\n",
    "\n",
    "# áp threshold\n",
    "y_pred_ensemble = (proba_ensemble >= thr).astype(int)\n",
    "\n",
    "print(\"\\n✓ Ensemble prediction distribution:\")\n",
    "print(f\"Class 0: {(y_pred_ensemble == 0).sum()} ({(y_pred_ensemble == 0).mean()*100:.2f}%)\")\n",
    "print(f\"Class 1: {(y_pred_ensemble == 1).sum()} ({(y_pred_ensemble == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "print(\n",
    "    \"\\nEnsemble proba stats:\",\n",
    "    f\"min={proba_ensemble.min():.6f}\",\n",
    "    f\"mean={proba_ensemble.mean():.6f}\",\n",
    "    f\"max={proba_ensemble.max():.6f}\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE SUBMISSION\n",
    "# ============================================================\n",
    "submission_ensemble = pd.DataFrame({\n",
    "    \"object_id\": test_object_ids,\n",
    "    \"prediction\": y_pred_ensemble\n",
    "})\n",
    "\n",
    "ensemble_path = \"/home/duy/Downloads/Mallorn_update/Mallorn/submission_ensemble.csv\"\n",
    "submission_ensemble.to_csv(ensemble_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Ensemble submission saved: {ensemble_path}\")\n",
    "print(submission_ensemble[\"prediction\"].value_counts())\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(submission_ensemble.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
