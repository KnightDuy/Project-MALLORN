{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c5e6ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước dữ liệu: (3043, 117)\n",
      "\n",
      "5 dòng đầu:\n",
      "                  object_id       Z  Z_err    EBV  target  u_n_obs  \\\n",
      "0  Dornhoth_fervain_onodrim  3.0490    NaN  0.110       0        5   \n",
      "1       Dornhoth_galadh_ylf  0.4324    NaN  0.058       0       15   \n",
      "2      Elrim_melethril_thul  0.4673    NaN  0.577       0        5   \n",
      "3        Ithil_tobas_rodwen  0.6946    NaN  0.012       0      108   \n",
      "4       Mirion_adar_Druadan  0.4161    NaN  0.058       0       13   \n",
      "\n",
      "   u_time_span  u_flux_mean  u_flux_std  u_flux_min  ...  color_mean_g_i  \\\n",
      "0     849.3841     0.584015    1.022628    0.000435  ...       -2.485906   \n",
      "1    2246.6157     0.027015    0.545088   -0.799254  ...       -0.173395   \n",
      "2     498.8939     0.006054    0.293108   -0.427984  ...       -2.060860   \n",
      "3    2858.4129     0.152980    0.439240   -1.455301  ...       -0.167220   \n",
      "4    1859.9219    -0.014067    0.406413   -0.828860  ...       -0.337740   \n",
      "\n",
      "   color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
      "0       -0.194878        -382.8829          44.0095         -44.0096   \n",
      "1       -0.252129        -821.6194          51.3512         -20.5406   \n",
      "2       -2.710143         444.6663           0.0000        -730.9880   \n",
      "3       -0.096281        1661.2019       -1331.8257        1331.8257   \n",
      "4       -0.086716         -48.5965        -234.1469        -108.2378   \n",
      "\n",
      "   t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
      "0         -13.2029         -17.6038        -338.8734          -0.0001   \n",
      "1         -64.1890         -20.5404        -770.2682          30.8106   \n",
      "2         110.6243        -147.4991         444.6663        -730.9880   \n",
      "3          25.7773       -1632.5606         329.3762           0.0000   \n",
      "4         474.9207        -523.5172        -282.7434        -342.3847   \n",
      "\n",
      "   t_peak_diff_r_z  \n",
      "0         -57.2125  \n",
      "1         -84.7296  \n",
      "2        -620.3637  \n",
      "3        1357.6030  \n",
      "4         366.6829  \n",
      "\n",
      "[5 rows x 117 columns]\n",
      "\n",
      "Thông tin cơ bản:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3043 entries, 0 to 3042\n",
      "Columns: 117 entries, object_id to t_peak_diff_r_z\n",
      "dtypes: float64(109), int64(7), object(1)\n",
      "memory usage: 2.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Đọc file CSV với pandas (khuyến nghị)\n",
    "df = pd.read_csv('/home/duy/Downloads/Mallorn_update/Mallorn/mallorn-astronomical-classification-challenge/train_features_ml.csv')\n",
    "print(\"Kích thước dữ liệu:\", df.shape)\n",
    "print(\"\\n5 dòng đầu:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nThông tin cơ bản:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "586343fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRƯỚC KHI XỬ LÝ NaN\n",
      "============================================================\n",
      "Kích thước DataFrame: (3043, 117)\n",
      "\n",
      "Tổng số NaN trong mỗi cột:\n",
      "Z_err              3043\n",
      "u_time_span          16\n",
      "u_flux_mean          41\n",
      "u_flux_std           41\n",
      "u_flux_min           41\n",
      "                   ... \n",
      "t_peak_diff_u_g      30\n",
      "t_peak_diff_g_r       2\n",
      "t_peak_diff_z_y      19\n",
      "t_peak_diff_u_r      28\n",
      "t_peak_diff_g_i       2\n",
      "Length: 92, dtype: int64\n",
      "\n",
      "Số hàng có ít nhất 1 NaN: 3043\n",
      "Tổng số NaN trong toàn bộ DataFrame: 6925\n",
      "\n",
      "============================================================\n",
      "XỬ LÝ NaN\n",
      "============================================================\n",
      "1. Điền giá trị 0 vào cột Z_err...\n",
      "   ✓ Đã điền 0 vào 3043 giá trị NaN trong Z_err\n",
      "\n",
      "2. Xóa các hàng có NaN ở các cột khác...\n",
      "\n",
      "============================================================\n",
      "SAU KHI XỬ LÝ NaN\n",
      "============================================================\n",
      "Kích thước DataFrame sau khi xử lý: (2876, 117)\n",
      "Số hàng đã xóa: 167\n",
      "% hàng đã xóa: 5.49%\n",
      "\n",
      "Còn NaN trong DataFrame không? False\n",
      "Tổng số NaN còn lại: 0\n",
      "\n",
      "Thống kê cột Z_err sau khi điền 0:\n",
      "  - Min: 0.0\n",
      "  - Max: 0.0\n",
      "  - Mean: 0.0000\n",
      "  - Số giá trị = 0: 2876\n",
      "\n",
      "✓ DataFrame đã được cập nhật!\n",
      "✓ Kích thước hiện tại: (2876, 117)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHECK VÀ XỬ LÝ CÁC GIÁ TRỊ NaN\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRƯỚC KHI XỬ LÝ NaN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Kích thước DataFrame: {df.shape}\")\n",
    "print(f\"\\nTổng số NaN trong mỗi cột:\")\n",
    "nan_counts = df.isnull().sum()\n",
    "print(nan_counts[nan_counts > 0])\n",
    "\n",
    "print(f\"\\nSố hàng có ít nhất 1 NaN: {df.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Tổng số NaN trong toàn bộ DataFrame: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# ============================================================\n",
    "# ĐIỀN GIÁ TRỊ 0 VÀO CỘT Z_err\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XỬ LÝ NaN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Điền 0 vào cột Z_err\n",
    "print(\"1. Điền giá trị 0 vào cột Z_err...\")\n",
    "df['Z_err'] = df['Z_err'].fillna(0)\n",
    "print(f\"   ✓ Đã điền 0 vào {nan_counts['Z_err']} giá trị NaN trong Z_err\")\n",
    "\n",
    "# Xóa các hàng có NaN ở các cột khác (không phải Z_err)\n",
    "print(\"\\n2. Xóa các hàng có NaN ở các cột khác...\")\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAU KHI XỬ LÝ NaN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Kích thước DataFrame sau khi xử lý: {df_cleaned.shape}\")\n",
    "print(f\"Số hàng đã xóa: {len(df) - len(df_cleaned)}\")\n",
    "print(f\"% hàng đã xóa: {((len(df) - len(df_cleaned)) / len(df) * 100):.2f}%\")\n",
    "\n",
    "# Kiểm tra xem còn NaN không\n",
    "print(f\"\\nCòn NaN trong DataFrame không? {df_cleaned.isnull().any().any()}\")\n",
    "print(f\"Tổng số NaN còn lại: {df_cleaned.isnull().sum().sum()}\")\n",
    "\n",
    "# Hiển thị thống kê cột Z_err sau khi điền\n",
    "print(f\"\\nThống kê cột Z_err sau khi điền 0:\")\n",
    "print(f\"  - Min: {df_cleaned['Z_err'].min()}\")\n",
    "print(f\"  - Max: {df_cleaned['Z_err'].max()}\")\n",
    "print(f\"  - Mean: {df_cleaned['Z_err'].mean():.4f}\")\n",
    "print(f\"  - Số giá trị = 0: {(df_cleaned['Z_err'] == 0).sum()}\")\n",
    "\n",
    "# Cập nhật df với bản đã clean\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "print(\"\\n✓ DataFrame đã được cập nhật!\")\n",
    "print(f\"✓ Kích thước hiện tại: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6366d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột sẽ được scaling:\n",
      "['Z', 'EBV', 'u_n_obs', 'u_time_span', 'u_flux_mean', 'u_flux_std', 'u_flux_min', 'u_flux_max', 'u_flux_median', 'u_amplitude'] ...\n",
      "Tổng số cột cần scaling: 114\n",
      "\n",
      "Thống kê trước khi scaling (5 cột đầu):\n",
      "                 Z          EBV      u_n_obs  u_time_span  u_flux_mean\n",
      "count  2876.000000  2876.000000  2876.000000  2876.000000  2876.000000\n",
      "mean      0.682277     0.053392    13.598053  1852.156176     0.393870\n",
      "std       0.542379     0.054949    15.015810   557.017193     1.590099\n",
      "min       0.008771     0.002000     3.000000    28.247300   -17.135965\n",
      "25%       0.324500     0.021000     9.000000  1481.598725     0.002790\n",
      "50%       0.493800     0.037000    12.000000  1863.958150     0.137668\n",
      "75%       0.896825     0.067000    14.000000  2219.633650     0.372800\n",
      "max       4.924000     0.758000   161.000000  3525.770100    50.071001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Tạo bản sao của DataFrame để không ảnh hưởng đến dữ liệu gốc\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Xác định các cột cần scaling (bỏ qua cột tên bệnh nhân và label)\n",
    "columns_to_scale = [col for col in df_scaled.columns \n",
    "                   if col not in ['object_id', 'Z_err', 'target']]\n",
    "\n",
    "print(\"Các cột sẽ được scaling:\")\n",
    "print(columns_to_scale[:10], \"...\")  # Hiển thị 10 cột đầu\n",
    "print(f\"Tổng số cột cần scaling: {len(columns_to_scale)}\")\n",
    "\n",
    "# Hiển thị thống kê trước khi scaling\n",
    "print(\"\\nThống kê trước khi scaling (5 cột đầu):\")\n",
    "print(df_scaled[columns_to_scale[:5]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34604743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>Z</th>\n",
       "      <th>Z_err</th>\n",
       "      <th>EBV</th>\n",
       "      <th>target</th>\n",
       "      <th>u_n_obs</th>\n",
       "      <th>u_time_span</th>\n",
       "      <th>u_flux_mean</th>\n",
       "      <th>u_flux_std</th>\n",
       "      <th>u_flux_min</th>\n",
       "      <th>...</th>\n",
       "      <th>color_mean_g_i</th>\n",
       "      <th>color_mean_r_z</th>\n",
       "      <th>t_peak_diff_u_g</th>\n",
       "      <th>t_peak_diff_g_r</th>\n",
       "      <th>t_peak_diff_r_i</th>\n",
       "      <th>t_peak_diff_i_z</th>\n",
       "      <th>t_peak_diff_z_y</th>\n",
       "      <th>t_peak_diff_u_r</th>\n",
       "      <th>t_peak_diff_g_i</th>\n",
       "      <th>t_peak_diff_r_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dornhoth_fervain_onodrim</td>\n",
       "      <td>3.0490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>849.3841</td>\n",
       "      <td>0.584015</td>\n",
       "      <td>1.022628</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.485906</td>\n",
       "      <td>-0.194878</td>\n",
       "      <td>-382.8829</td>\n",
       "      <td>44.0095</td>\n",
       "      <td>-44.0096</td>\n",
       "      <td>-13.2029</td>\n",
       "      <td>-17.6038</td>\n",
       "      <td>-338.8734</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-57.2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dornhoth_galadh_ylf</td>\n",
       "      <td>0.4324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2246.6157</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.545088</td>\n",
       "      <td>-0.799254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173395</td>\n",
       "      <td>-0.252129</td>\n",
       "      <td>-821.6194</td>\n",
       "      <td>51.3512</td>\n",
       "      <td>-20.5406</td>\n",
       "      <td>-64.1890</td>\n",
       "      <td>-20.5404</td>\n",
       "      <td>-770.2682</td>\n",
       "      <td>30.8106</td>\n",
       "      <td>-84.7296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ithil_tobas_rodwen</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>2858.4129</td>\n",
       "      <td>0.152980</td>\n",
       "      <td>0.439240</td>\n",
       "      <td>-1.455301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167220</td>\n",
       "      <td>-0.096281</td>\n",
       "      <td>1661.2019</td>\n",
       "      <td>-1331.8257</td>\n",
       "      <td>1331.8257</td>\n",
       "      <td>25.7773</td>\n",
       "      <td>-1632.5606</td>\n",
       "      <td>329.3762</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1357.6030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mirion_adar_Druadan</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1859.9219</td>\n",
       "      <td>-0.014067</td>\n",
       "      <td>0.406413</td>\n",
       "      <td>-0.828860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337740</td>\n",
       "      <td>-0.086716</td>\n",
       "      <td>-48.5965</td>\n",
       "      <td>-234.1469</td>\n",
       "      <td>-108.2378</td>\n",
       "      <td>474.9207</td>\n",
       "      <td>-523.5172</td>\n",
       "      <td>-282.7434</td>\n",
       "      <td>-342.3847</td>\n",
       "      <td>366.6829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mirion_lalaith_neledh</td>\n",
       "      <td>1.1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1832.8097</td>\n",
       "      <td>0.422299</td>\n",
       "      <td>0.288031</td>\n",
       "      <td>-0.023856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394888</td>\n",
       "      <td>-0.449806</td>\n",
       "      <td>-99.0709</td>\n",
       "      <td>1183.0219</td>\n",
       "      <td>-1177.1941</td>\n",
       "      <td>1369.5080</td>\n",
       "      <td>-1427.7850</td>\n",
       "      <td>1083.9510</td>\n",
       "      <td>5.8278</td>\n",
       "      <td>192.3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>tinnu_gellui_tathar</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2189.5612</td>\n",
       "      <td>-0.068665</td>\n",
       "      <td>0.395604</td>\n",
       "      <td>-0.825168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309896</td>\n",
       "      <td>-0.412576</td>\n",
       "      <td>2085.4235</td>\n",
       "      <td>-595.4538</td>\n",
       "      <td>283.0408</td>\n",
       "      <td>-197.5945</td>\n",
       "      <td>-1746.3085</td>\n",
       "      <td>1489.9697</td>\n",
       "      <td>-312.4130</td>\n",
       "      <td>85.4463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>uir_heleg_corf</td>\n",
       "      <td>0.9598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2887.8277</td>\n",
       "      <td>1.134710</td>\n",
       "      <td>2.021232</td>\n",
       "      <td>-2.843739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342609</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-12.5968</td>\n",
       "      <td>25.1937</td>\n",
       "      <td>176.3559</td>\n",
       "      <td>-289.7275</td>\n",
       "      <td>-12.5968</td>\n",
       "      <td>12.5969</td>\n",
       "      <td>201.5496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>uir_rhosc_law</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1829.4130</td>\n",
       "      <td>0.041323</td>\n",
       "      <td>0.481292</td>\n",
       "      <td>-0.979264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361688</td>\n",
       "      <td>-0.500247</td>\n",
       "      <td>60.7013</td>\n",
       "      <td>-4.1863</td>\n",
       "      <td>-35.5835</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0932</td>\n",
       "      <td>56.5150</td>\n",
       "      <td>-39.7698</td>\n",
       "      <td>-35.5835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>uruk_in_pess</td>\n",
       "      <td>1.1520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2568.6786</td>\n",
       "      <td>0.703543</td>\n",
       "      <td>1.046953</td>\n",
       "      <td>-2.024119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360625</td>\n",
       "      <td>-0.177830</td>\n",
       "      <td>-803.5525</td>\n",
       "      <td>732.9475</td>\n",
       "      <td>-396.7331</td>\n",
       "      <td>245.4366</td>\n",
       "      <td>581.6510</td>\n",
       "      <td>-70.6050</td>\n",
       "      <td>336.2144</td>\n",
       "      <td>-151.2965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>ylf_alph_mindon</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1719.8078</td>\n",
       "      <td>-0.189906</td>\n",
       "      <td>0.238451</td>\n",
       "      <td>-0.585085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232245</td>\n",
       "      <td>-0.032634</td>\n",
       "      <td>917.8616</td>\n",
       "      <td>-286.2403</td>\n",
       "      <td>21.2906</td>\n",
       "      <td>-2.3656</td>\n",
       "      <td>-26.0219</td>\n",
       "      <td>631.6213</td>\n",
       "      <td>-264.9497</td>\n",
       "      <td>18.9250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2876 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     object_id       Z  Z_err    EBV  target  u_n_obs  \\\n",
       "0     Dornhoth_fervain_onodrim  3.0490    0.0  0.110       0        5   \n",
       "1          Dornhoth_galadh_ylf  0.4324    0.0  0.058       0       15   \n",
       "3           Ithil_tobas_rodwen  0.6946    0.0  0.012       0      108   \n",
       "4          Mirion_adar_Druadan  0.4161    0.0  0.058       0       13   \n",
       "5        Mirion_lalaith_neledh  1.1970    0.0  0.054       0        8   \n",
       "...                        ...     ...    ...    ...     ...      ...   \n",
       "3038       tinnu_gellui_tathar  0.8898    0.0  0.042       0       16   \n",
       "3039            uir_heleg_corf  0.9598    0.0  0.042       0       13   \n",
       "3040             uir_rhosc_law  0.1543    0.0  0.024       0       16   \n",
       "3041              uruk_in_pess  1.1520    0.0  0.019       0       13   \n",
       "3042           ylf_alph_mindon  0.5595    0.0  0.034       0       17   \n",
       "\n",
       "      u_time_span  u_flux_mean  u_flux_std  u_flux_min  ...  color_mean_g_i  \\\n",
       "0        849.3841     0.584015    1.022628    0.000435  ...       -2.485906   \n",
       "1       2246.6157     0.027015    0.545088   -0.799254  ...       -0.173395   \n",
       "3       2858.4129     0.152980    0.439240   -1.455301  ...       -0.167220   \n",
       "4       1859.9219    -0.014067    0.406413   -0.828860  ...       -0.337740   \n",
       "5       1832.8097     0.422299    0.288031   -0.023856  ...       -0.394888   \n",
       "...           ...          ...         ...         ...  ...             ...   \n",
       "3038    2189.5612    -0.068665    0.395604   -0.825168  ...       -0.309896   \n",
       "3039    2887.8277     1.134710    2.021232   -2.843739  ...        0.342609   \n",
       "3040    1829.4130     0.041323    0.481292   -0.979264  ...       -0.361688   \n",
       "3041    2568.6786     0.703543    1.046953   -2.024119  ...        0.360625   \n",
       "3042    1719.8078    -0.189906    0.238451   -0.585085  ...       -0.232245   \n",
       "\n",
       "      color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
       "0          -0.194878        -382.8829          44.0095         -44.0096   \n",
       "1          -0.252129        -821.6194          51.3512         -20.5406   \n",
       "3          -0.096281        1661.2019       -1331.8257        1331.8257   \n",
       "4          -0.086716         -48.5965        -234.1469        -108.2378   \n",
       "5          -0.449806         -99.0709        1183.0219       -1177.1941   \n",
       "...              ...              ...              ...              ...   \n",
       "3038       -0.412576        2085.4235        -595.4538         283.0408   \n",
       "3039        0.074489           0.0000         -12.5968          25.1937   \n",
       "3040       -0.500247          60.7013          -4.1863         -35.5835   \n",
       "3041       -0.177830        -803.5525         732.9475        -396.7331   \n",
       "3042       -0.032634         917.8616        -286.2403          21.2906   \n",
       "\n",
       "      t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
       "0            -13.2029         -17.6038        -338.8734          -0.0001   \n",
       "1            -64.1890         -20.5404        -770.2682          30.8106   \n",
       "3             25.7773       -1632.5606         329.3762           0.0000   \n",
       "4            474.9207        -523.5172        -282.7434        -342.3847   \n",
       "5           1369.5080       -1427.7850        1083.9510           5.8278   \n",
       "...               ...              ...              ...              ...   \n",
       "3038        -197.5945       -1746.3085        1489.9697        -312.4130   \n",
       "3039         176.3559        -289.7275         -12.5968          12.5969   \n",
       "3040           0.0000          -2.0932          56.5150         -39.7698   \n",
       "3041         245.4366         581.6510         -70.6050         336.2144   \n",
       "3042          -2.3656         -26.0219         631.6213        -264.9497   \n",
       "\n",
       "      t_peak_diff_r_z  \n",
       "0            -57.2125  \n",
       "1            -84.7296  \n",
       "3           1357.6030  \n",
       "4            366.6829  \n",
       "5            192.3139  \n",
       "...               ...  \n",
       "3038          85.4463  \n",
       "3039         201.5496  \n",
       "3040         -35.5835  \n",
       "3041        -151.2965  \n",
       "3042          18.9250  \n",
       "\n",
       "[2876 rows x 117 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c97dacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Có NaN trong df không? False\n",
      "\n",
      "Số NaN trong mỗi cột:\n",
      "object_id          0\n",
      "Z                  0\n",
      "Z_err              0\n",
      "EBV                0\n",
      "target             0\n",
      "                  ..\n",
      "t_peak_diff_i_z    0\n",
      "t_peak_diff_z_y    0\n",
      "t_peak_diff_u_r    0\n",
      "t_peak_diff_g_i    0\n",
      "t_peak_diff_r_z    0\n",
      "Length: 117, dtype: int64\n",
      "\n",
      "% NaN trong mỗi cột:\n",
      "object_id          0.0\n",
      "Z                  0.0\n",
      "Z_err              0.0\n",
      "EBV                0.0\n",
      "target             0.0\n",
      "                  ... \n",
      "t_peak_diff_i_z    0.0\n",
      "t_peak_diff_z_y    0.0\n",
      "t_peak_diff_u_r    0.0\n",
      "t_peak_diff_g_i    0.0\n",
      "t_peak_diff_r_z    0.0\n",
      "Length: 117, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra xem có NaN không\n",
    "print(\"Có NaN trong df không?\", df.isnull().any().any())\n",
    "\n",
    "# Đếm số NaN trong mỗi cột\n",
    "print(\"\\nSố NaN trong mỗi cột:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Tính % NaN trong mỗi cột\n",
    "print(\"\\n% NaN trong mỗi cột:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88462019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaling đã hoàn thành!\n",
      "\n",
      "Thống kê sau khi scaling (5 cột đầu):\n",
      "                 Z          EBV      u_n_obs  u_time_span  u_flux_mean\n",
      "count  2876.000000  2876.000000  2876.000000  2876.000000  2876.000000\n",
      "mean      0.137024     0.067979     0.067076     0.521486     0.260834\n",
      "std       0.110347     0.072683     0.095037     0.159260     0.023660\n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%       0.064235     0.025132     0.037975     0.415537     0.255015\n",
      "50%       0.098679     0.046296     0.056962     0.524860     0.257021\n",
      "75%       0.180674     0.085979     0.069620     0.626554     0.260520\n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000\n",
      "\n",
      "Phạm vi giá trị sau scaling:\n",
      "Min: 0.000000\n",
      "Max: 1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scaling các cột đã chọn\n",
    "df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n",
    "\n",
    "print(\"Min-Max Scaling đã hoàn thành!\")\n",
    "print(\"\\nThống kê sau khi scaling (5 cột đầu):\")\n",
    "print(df_scaled[columns_to_scale[:5]].describe())\n",
    "\n",
    "# Kiểm tra phạm vi giá trị sau scaling\n",
    "print(f\"\\nPhạm vi giá trị sau scaling:\")\n",
    "print(f\"Min: {df_scaled[columns_to_scale].min().min():.6f}\")\n",
    "print(f\"Max: {df_scaled[columns_to_scale].max().max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c06ec40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>Z</th>\n",
       "      <th>Z_err</th>\n",
       "      <th>EBV</th>\n",
       "      <th>target</th>\n",
       "      <th>u_n_obs</th>\n",
       "      <th>u_time_span</th>\n",
       "      <th>u_flux_mean</th>\n",
       "      <th>u_flux_std</th>\n",
       "      <th>u_flux_min</th>\n",
       "      <th>...</th>\n",
       "      <th>color_mean_g_i</th>\n",
       "      <th>color_mean_r_z</th>\n",
       "      <th>t_peak_diff_u_g</th>\n",
       "      <th>t_peak_diff_g_r</th>\n",
       "      <th>t_peak_diff_r_i</th>\n",
       "      <th>t_peak_diff_i_z</th>\n",
       "      <th>t_peak_diff_z_y</th>\n",
       "      <th>t_peak_diff_u_r</th>\n",
       "      <th>t_peak_diff_g_i</th>\n",
       "      <th>t_peak_diff_r_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dornhoth_fervain_onodrim</td>\n",
       "      <td>0.618533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.234777</td>\n",
       "      <td>0.263663</td>\n",
       "      <td>0.025473</td>\n",
       "      <td>0.701691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220216</td>\n",
       "      <td>0.430231</td>\n",
       "      <td>0.461601</td>\n",
       "      <td>0.625388</td>\n",
       "      <td>0.489657</td>\n",
       "      <td>0.481269</td>\n",
       "      <td>0.508584</td>\n",
       "      <td>0.501736</td>\n",
       "      <td>0.515684</td>\n",
       "      <td>0.472637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dornhoth_galadh_ylf</td>\n",
       "      <td>0.086187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.634268</td>\n",
       "      <td>0.255375</td>\n",
       "      <td>0.013209</td>\n",
       "      <td>0.684591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325172</td>\n",
       "      <td>0.426179</td>\n",
       "      <td>0.378973</td>\n",
       "      <td>0.626645</td>\n",
       "      <td>0.494262</td>\n",
       "      <td>0.471119</td>\n",
       "      <td>0.507978</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.521944</td>\n",
       "      <td>0.467464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ithil_tobas_rodwen</td>\n",
       "      <td>0.139531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013228</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.257249</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.670563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325453</td>\n",
       "      <td>0.437208</td>\n",
       "      <td>0.846568</td>\n",
       "      <td>0.389680</td>\n",
       "      <td>0.759607</td>\n",
       "      <td>0.489029</td>\n",
       "      <td>0.175839</td>\n",
       "      <td>0.635187</td>\n",
       "      <td>0.515684</td>\n",
       "      <td>0.738604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mirion_adar_Druadan</td>\n",
       "      <td>0.082871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.523706</td>\n",
       "      <td>0.254764</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.683958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317713</td>\n",
       "      <td>0.437885</td>\n",
       "      <td>0.524558</td>\n",
       "      <td>0.577734</td>\n",
       "      <td>0.477055</td>\n",
       "      <td>0.578441</td>\n",
       "      <td>0.404345</td>\n",
       "      <td>0.512945</td>\n",
       "      <td>0.446111</td>\n",
       "      <td>0.552323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mirion_lalaith_neledh</td>\n",
       "      <td>0.241744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.515954</td>\n",
       "      <td>0.261257</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.701172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315120</td>\n",
       "      <td>0.412191</td>\n",
       "      <td>0.515052</td>\n",
       "      <td>0.820523</td>\n",
       "      <td>0.267317</td>\n",
       "      <td>0.756529</td>\n",
       "      <td>0.218031</td>\n",
       "      <td>0.785879</td>\n",
       "      <td>0.516868</td>\n",
       "      <td>0.519544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>tinnu_gellui_tathar</td>\n",
       "      <td>0.179245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082278</td>\n",
       "      <td>0.617956</td>\n",
       "      <td>0.253951</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318977</td>\n",
       "      <td>0.414826</td>\n",
       "      <td>0.926462</td>\n",
       "      <td>0.515835</td>\n",
       "      <td>0.553827</td>\n",
       "      <td>0.444562</td>\n",
       "      <td>0.152402</td>\n",
       "      <td>0.866962</td>\n",
       "      <td>0.452201</td>\n",
       "      <td>0.499455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>uir_heleg_corf</td>\n",
       "      <td>0.193486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.817602</td>\n",
       "      <td>0.271857</td>\n",
       "      <td>0.051119</td>\n",
       "      <td>0.640874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348592</td>\n",
       "      <td>0.449292</td>\n",
       "      <td>0.533710</td>\n",
       "      <td>0.615690</td>\n",
       "      <td>0.503236</td>\n",
       "      <td>0.519005</td>\n",
       "      <td>0.452515</td>\n",
       "      <td>0.566894</td>\n",
       "      <td>0.518243</td>\n",
       "      <td>0.521281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>uir_rhosc_law</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082278</td>\n",
       "      <td>0.514983</td>\n",
       "      <td>0.255588</td>\n",
       "      <td>0.011571</td>\n",
       "      <td>0.680742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316626</td>\n",
       "      <td>0.408622</td>\n",
       "      <td>0.545142</td>\n",
       "      <td>0.617131</td>\n",
       "      <td>0.491311</td>\n",
       "      <td>0.483898</td>\n",
       "      <td>0.511779</td>\n",
       "      <td>0.580696</td>\n",
       "      <td>0.507602</td>\n",
       "      <td>0.476703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>uruk_in_pess</td>\n",
       "      <td>0.232589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022487</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.726352</td>\n",
       "      <td>0.265441</td>\n",
       "      <td>0.026098</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349410</td>\n",
       "      <td>0.431437</td>\n",
       "      <td>0.382375</td>\n",
       "      <td>0.743416</td>\n",
       "      <td>0.420450</td>\n",
       "      <td>0.532757</td>\n",
       "      <td>0.632054</td>\n",
       "      <td>0.555310</td>\n",
       "      <td>0.584002</td>\n",
       "      <td>0.454950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>ylf_alph_mindon</td>\n",
       "      <td>0.112045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042328</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.483645</td>\n",
       "      <td>0.252147</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>0.689171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322501</td>\n",
       "      <td>0.441712</td>\n",
       "      <td>0.706573</td>\n",
       "      <td>0.568809</td>\n",
       "      <td>0.502470</td>\n",
       "      <td>0.483427</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>0.695547</td>\n",
       "      <td>0.461846</td>\n",
       "      <td>0.486949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2876 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     object_id         Z  Z_err       EBV  target   u_n_obs  \\\n",
       "0     Dornhoth_fervain_onodrim  0.618533    0.0  0.142857       0  0.012658   \n",
       "1          Dornhoth_galadh_ylf  0.086187    0.0  0.074074       0  0.075949   \n",
       "3           Ithil_tobas_rodwen  0.139531    0.0  0.013228       0  0.664557   \n",
       "4          Mirion_adar_Druadan  0.082871    0.0  0.074074       0  0.063291   \n",
       "5        Mirion_lalaith_neledh  0.241744    0.0  0.068783       0  0.031646   \n",
       "...                        ...       ...    ...       ...     ...       ...   \n",
       "3038       tinnu_gellui_tathar  0.179245    0.0  0.052910       0  0.082278   \n",
       "3039            uir_heleg_corf  0.193486    0.0  0.052910       0  0.063291   \n",
       "3040             uir_rhosc_law  0.029608    0.0  0.029101       0  0.082278   \n",
       "3041              uruk_in_pess  0.232589    0.0  0.022487       0  0.063291   \n",
       "3042           ylf_alph_mindon  0.112045    0.0  0.042328       0  0.088608   \n",
       "\n",
       "      u_time_span  u_flux_mean  u_flux_std  u_flux_min  ...  color_mean_g_i  \\\n",
       "0        0.234777     0.263663    0.025473    0.701691  ...        0.220216   \n",
       "1        0.634268     0.255375    0.013209    0.684591  ...        0.325172   \n",
       "3        0.809191     0.257249    0.010491    0.670563  ...        0.325453   \n",
       "4        0.523706     0.254764    0.009648    0.683958  ...        0.317713   \n",
       "5        0.515954     0.261257    0.006607    0.701172  ...        0.315120   \n",
       "...           ...          ...         ...         ...  ...             ...   \n",
       "3038     0.617956     0.253951    0.009370    0.684037  ...        0.318977   \n",
       "3039     0.817602     0.271857    0.051119    0.640874  ...        0.348592   \n",
       "3040     0.514983     0.255588    0.011571    0.680742  ...        0.316626   \n",
       "3041     0.726352     0.265441    0.026098    0.658400  ...        0.349410   \n",
       "3042     0.483645     0.252147    0.005334    0.689171  ...        0.322501   \n",
       "\n",
       "      color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
       "0           0.430231         0.461601         0.625388         0.489657   \n",
       "1           0.426179         0.378973         0.626645         0.494262   \n",
       "3           0.437208         0.846568         0.389680         0.759607   \n",
       "4           0.437885         0.524558         0.577734         0.477055   \n",
       "5           0.412191         0.515052         0.820523         0.267317   \n",
       "...              ...              ...              ...              ...   \n",
       "3038        0.414826         0.926462         0.515835         0.553827   \n",
       "3039        0.449292         0.533710         0.615690         0.503236   \n",
       "3040        0.408622         0.545142         0.617131         0.491311   \n",
       "3041        0.431437         0.382375         0.743416         0.420450   \n",
       "3042        0.441712         0.706573         0.568809         0.502470   \n",
       "\n",
       "      t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
       "0            0.481269         0.508584         0.501736         0.515684   \n",
       "1            0.471119         0.507978         0.415584         0.521944   \n",
       "3            0.489029         0.175839         0.635187         0.515684   \n",
       "4            0.578441         0.404345         0.512945         0.446111   \n",
       "5            0.756529         0.218031         0.785879         0.516868   \n",
       "...               ...              ...              ...              ...   \n",
       "3038         0.444562         0.152402         0.866962         0.452201   \n",
       "3039         0.519005         0.452515         0.566894         0.518243   \n",
       "3040         0.483898         0.511779         0.580696         0.507602   \n",
       "3041         0.532757         0.632054         0.555310         0.584002   \n",
       "3042         0.483427         0.506849         0.695547         0.461846   \n",
       "\n",
       "      t_peak_diff_r_z  \n",
       "0            0.472637  \n",
       "1            0.467464  \n",
       "3            0.738604  \n",
       "4            0.552323  \n",
       "5            0.519544  \n",
       "...               ...  \n",
       "3038         0.499455  \n",
       "3039         0.521281  \n",
       "3040         0.476703  \n",
       "3041         0.454950  \n",
       "3042         0.486949  \n",
       "\n",
       "[2876 rows x 117 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3217d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING y_base\n",
      "============================================================\n",
      "y_base type: <class 'pandas.core.series.Series'>\n",
      "y_base shape: (2876,)\n",
      "y_base ndim: 1\n",
      "First 10 values:\n",
      "0     0\n",
      "1     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "Name: target, dtype: int64\n",
      "Value counts:\n",
      "target\n",
      "0    2729\n",
      "1     147\n",
      "Name: count, dtype: int64\n",
      "\n",
      "X_base_features shape: (2876, 114)\n"
     ]
    }
   ],
   "source": [
    "X_base = df_scaled.copy()\n",
    "X_base.drop(['Z_err'], axis=1, inplace=True)\n",
    "y_base = X_base['target'].copy()\n",
    "\n",
    "# KIỂM TRA y_base\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKING y_base\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"y_base type: {type(y_base)}\")\n",
    "print(f\"y_base shape: {y_base.shape}\")\n",
    "print(f\"y_base ndim: {y_base.ndim}\")\n",
    "print(f\"First 10 values:\\n{y_base.head(10)}\")\n",
    "print(f\"Value counts:\\n{y_base.value_counts()}\")\n",
    "\n",
    "exclude_cols = ['object_id', 'Z_err', 'target']\n",
    "feature_cols_base = [col for col in X_base.columns \n",
    "                     if col not in exclude_cols \n",
    "                     and X_base[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "X_base_features = X_base[feature_cols_base].copy()\n",
    "print(f\"\\nX_base_features shape: {X_base_features.shape}\")\n",
    "# X_base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f05ad8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số hàng có NaN: 0\n",
      "\n",
      "Các hàng có NaN:\n",
      "Empty DataFrame\n",
      "Columns: [object_id, Z, Z_err, EBV, target, u_n_obs, u_time_span, u_flux_mean, u_flux_std, u_flux_min, u_flux_max, u_flux_median, u_amplitude, u_flux_skew, u_flux_kurtosis, u_t_peak, u_rise_time, u_decay_time, u_max_slope_up, u_max_slope_down, u_auc, g_n_obs, g_time_span, g_flux_mean, g_flux_std, g_flux_min, g_flux_max, g_flux_median, g_amplitude, g_flux_skew, g_flux_kurtosis, g_t_peak, g_rise_time, g_decay_time, g_max_slope_up, g_max_slope_down, g_auc, r_n_obs, r_time_span, r_flux_mean, r_flux_std, r_flux_min, r_flux_max, r_flux_median, r_amplitude, r_flux_skew, r_flux_kurtosis, r_t_peak, r_rise_time, r_decay_time, r_max_slope_up, r_max_slope_down, r_auc, i_n_obs, i_time_span, i_flux_mean, i_flux_std, i_flux_min, i_flux_max, i_flux_median, i_amplitude, i_flux_skew, i_flux_kurtosis, i_t_peak, i_rise_time, i_decay_time, i_max_slope_up, i_max_slope_down, i_auc, z_n_obs, z_time_span, z_flux_mean, z_flux_std, z_flux_min, z_flux_max, z_flux_median, z_amplitude, z_flux_skew, z_flux_kurtosis, z_t_peak, z_rise_time, z_decay_time, z_max_slope_up, z_max_slope_down, z_auc, y_n_obs, y_time_span, y_flux_mean, y_flux_std, y_flux_min, y_flux_max, y_flux_median, y_amplitude, y_flux_skew, y_flux_kurtosis, y_t_peak, y_rise_time, y_decay_time, y_max_slope_up, y_max_slope_down, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 117 columns]\n",
      "\n",
      "Index của các hàng có NaN: []\n"
     ]
    }
   ],
   "source": [
    "rows_with_nan = X_base.isnull().any(axis=1)\n",
    "print(\"Số hàng có NaN:\", rows_with_nan.sum())\n",
    "\n",
    "# Lấy ra các hàng có NaN\n",
    "df_with_nan = df[rows_with_nan]\n",
    "print(\"\\nCác hàng có NaN:\")\n",
    "print(df_with_nan)\n",
    "\n",
    "# Lấy index của các hàng có NaN\n",
    "nan_indices = df[rows_with_nan].index.tolist()\n",
    "print(\"\\nIndex của các hàng có NaN:\", nan_indices[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a30df916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix,\n",
    "                             classification_report, roc_curve)\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import models\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              BaggingClassifier, AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56e2f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    num_classes = cm.shape[0]\n",
    "    specificity_list = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # TN = tổng tất cả phần tử ngoại trừ hàng i và cột i\n",
    "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
    "        # FP = tổng cột i (ngoại trừ phần tử đúng)\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "        specificity_list.append(specificity)\n",
    "\n",
    "    # trung bình macro (mỗi lớp có trọng số bằng nhau)\n",
    "    return np.mean(specificity_list)\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    specificity = calculate_specificity(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        # Binary classification: chỉ lấy xác suất của class dương (class 1)\n",
    "        if y_pred_proba.shape[1] == 2:\n",
    "            auc_roc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            auc_roc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    else:\n",
    "        auc_roc = np.nan\n",
    "        \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Specificity': specificity,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1 Score': f1,\n",
    "        'AUC': auc_roc\n",
    "    }\n",
    "    return results, y_pred, y_pred_proba\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=5, use_stratified=True):\n",
    "    \"\"\"\n",
    "    Thực hiện k-fold cross-validation với đầy đủ metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Model cần đánh giá\n",
    "    X : array-like\n",
    "        Features\n",
    "    y : array-like\n",
    "        Labels\n",
    "    cv : int\n",
    "        Số lượng folds (mặc định 5)\n",
    "    use_stratified : bool\n",
    "        Sử dụng StratifiedKFold để đảm bảo phân phối lớp (mặc định True)\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    if use_stratified:\n",
    "        kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Tính toán các metrics cho từng fold\n",
    "    fold_accuracies = []\n",
    "    fold_recalls = []\n",
    "    fold_precisions = []\n",
    "    fold_f1s = []\n",
    "    fold_specificities = []\n",
    "    fold_aucs = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model_copy = clone(model)\n",
    "        model_copy.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_fold = model_copy.predict(X_val_fold)\n",
    "        y_pred_proba_fold = model_copy.predict_proba(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fold_accuracies.append(accuracy_score(y_val_fold, y_pred_fold))\n",
    "        fold_recalls.append(recall_score(y_val_fold, y_pred_fold, average='macro', zero_division=0))\n",
    "        fold_precisions.append(precision_score(y_val_fold, y_pred_fold, average='macro', zero_division=0))\n",
    "        fold_f1s.append(f1_score(y_val_fold, y_pred_fold, average='macro', zero_division=0))\n",
    "        fold_specificities.append(calculate_specificity(y_val_fold, y_pred_fold))\n",
    "        \n",
    "        try:\n",
    "            # Binary classification: chỉ lấy xác suất của class dương (class 1)\n",
    "            if y_pred_proba_fold.shape[1] == 2:\n",
    "                fold_aucs.append(roc_auc_score(y_val_fold, y_pred_proba_fold[:, 1]))\n",
    "            else:\n",
    "                # Multi-class classification\n",
    "                fold_aucs.append(roc_auc_score(y_val_fold, y_pred_proba_fold, multi_class='ovr', average='macro'))\n",
    "        except:\n",
    "            fold_aucs.append(np.nan)\n",
    "    \n",
    "    # Tính mean và std cho tất cả metrics\n",
    "    cv_results = {\n",
    "        'CV_Accuracy_mean': np.mean(fold_accuracies),\n",
    "        'CV_Accuracy_std': np.std(fold_accuracies),\n",
    "        'CV_Recall_mean': np.mean(fold_recalls),\n",
    "        'CV_Recall_std': np.std(fold_recalls),\n",
    "        'CV_Precision_mean': np.mean(fold_precisions),\n",
    "        'CV_Precision_std': np.std(fold_precisions),\n",
    "        'CV_F1_mean': np.mean(fold_f1s),\n",
    "        'CV_F1_std': np.std(fold_f1s),\n",
    "        'CV_Specificity_mean': np.mean(fold_specificities),\n",
    "        'CV_Specificity_std': np.std(fold_specificities),\n",
    "        'CV_AUC_mean': np.nanmean(fold_aucs),\n",
    "        'CV_AUC_std': np.nanstd(fold_aucs),\n",
    "        'CV_Fold_Details': {\n",
    "            'accuracies': fold_accuracies,\n",
    "            'recalls': fold_recalls,\n",
    "            'precisions': fold_precisions,\n",
    "            'f1s': fold_f1s,\n",
    "            'specificities': fold_specificities,\n",
    "            'aucs': fold_aucs\n",
    "        }\n",
    "    }\n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "796795a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Train-Test Split (80:20)\n",
      "============================================================\n",
      "y_base type: <class 'pandas.core.series.Series'>\n",
      "y_base shape: (2876,)\n",
      "y_base dtype: int64\n",
      "y_base first 5 values:\n",
      "0    0\n",
      "1    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "Name: target, dtype: int64\n",
      "\n",
      "y_train type: <class 'pandas.core.series.Series'>\n",
      "y_train shape: (2300,)\n",
      "y_test type: <class 'pandas.core.series.Series'>\n",
      "y_test shape: (576,)\n",
      "\n",
      "Train set shape: (2300, 114)\n",
      "Test set shape: (576, 114)\n",
      "Train set size: 2300 samples (80.0%)\n",
      "Test set size: 576 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Train-Test Split (80:20)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# KIỂM TRA y_base trước khi split\n",
    "print(f\"y_base type: {type(y_base)}\")\n",
    "print(f\"y_base shape: {y_base.shape}\")\n",
    "print(f\"y_base dtype: {y_base.dtype}\")\n",
    "print(f\"y_base first 5 values:\\n{y_base.head()}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_base_features, y_base, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_base\n",
    ")\n",
    "\n",
    "# KIỂM TRA sau khi split\n",
    "print(f\"\\ny_train type: {type(y_train)}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test type: {type(y_test)}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTrain set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Train set size: {len(X_train)} samples ({len(X_train)/len(X_base_features)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(X_base_features)*100:.1f}%)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9e67e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEFINING MODELS WITH HYPERPARAMETERS\n",
      "================================================================================\n",
      "Total models to evaluate: 2\n",
      "  - LR\n",
      "  - LightGBM\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEFINING MODELS WITH HYPERPARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "models = {\n",
    "    'LR': LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\"\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        is_unbalance=True\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Total models to evaluate: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eaa58307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING AND EVALUATING MODELS\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training LR...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for LR...\n",
      "Test Accuracy: 0.7274\n",
      "Test AUC: 0.7780\n",
      "CV Accuracy: 0.7087 (+/- 0.0090)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training LightGBM...\n",
      "--------------------------------------------------------------------------------\n",
      "Performing 5-Fold Cross-Validation for LightGBM...\n",
      "Test Accuracy: 0.9462\n",
      "Test AUC: 0.9315\n",
      "CV Accuracy: 0.9543 (+/- 0.0051)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING AND EVALUATING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = []\n",
    "detailed_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    results, y_pred, y_pred_proba = evaluate_model(\n",
    "        model, X_train, X_test, y_train, y_test, model_name\n",
    "    )\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    print(f\"Performing 5-Fold Cross-Validation for {model_name}...\")\n",
    "    cv_results = cross_validate_model(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Combine results\n",
    "    results.update(cv_results)\n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Store detailed results\n",
    "    detailed_results[model_name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Test Accuracy: {results['Accuracy']:.4f}\")\n",
    "    print(f\"Test AUC: {results['AUC']:.4f}\")\n",
    "    print(f\"CV Accuracy: {results['CV_Accuracy_mean']:.4f} (+/- {results['CV_Accuracy_std']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số feature ban đầu: 114\n",
      "Số feature sau khi chọn: 45\n",
      "Các feature được chọn:\n",
      "Index(['Z', 'u_flux_mean', 'u_flux_std', 'u_flux_max', 'u_flux_median',\n",
      "       'u_amplitude', 'u_decay_time', 'u_auc', 'g_n_obs', 'g_flux_mean',\n",
      "       'g_flux_std', 'g_flux_min', 'g_flux_max', 'g_flux_skew',\n",
      "       'g_flux_kurtosis', 'g_decay_time', 'g_max_slope_up', 'g_auc',\n",
      "       'r_flux_std', 'r_flux_min', 'r_flux_max', 'r_flux_median',\n",
      "       'r_amplitude', 'r_flux_skew', 'r_flux_kurtosis', 'r_t_peak',\n",
      "       'r_max_slope_up', 'r_max_slope_down', 'r_auc', 'i_flux_std',\n",
      "       'i_flux_max', 'i_amplitude', 'i_flux_skew', 'i_decay_time', 'i_auc',\n",
      "       'z_time_span', 'z_t_peak', 'z_max_slope_down', 'y_time_span',\n",
      "       'y_flux_min', 'color_mean_r_i', 'color_mean_u_r', 'color_mean_g_i',\n",
      "       'color_mean_r_z', 't_peak_diff_u_g'],\n",
      "      dtype='object')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       547\n",
      "           1       0.50      0.14      0.22        29\n",
      "\n",
      "    accuracy                           0.95       576\n",
      "   macro avg       0.73      0.57      0.60       576\n",
      "weighted avg       0.93      0.95      0.94       576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Khai báo model XGBoost cơ bản\n",
    "xgb_base = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=2,          # nếu 3 lớp\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# 2. RFE: chọn k features quan trọng nhất\n",
    "n_features_to_select = 114   # mày chỉnh số này cho phù hợp\n",
    "\n",
    "rfe = RFE(\n",
    "    estimator=xgb_base,\n",
    "    n_features_to_select=n_features_to_select,\n",
    "    step=1\n",
    ")\n",
    "\n",
    "# 3. Fit RFE trên train\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "print(\"Số feature ban đầu:\", X_train.shape[1])\n",
    "print(\"Số feature sau khi chọn:\", rfe.n_features_)\n",
    "\n",
    "# 4. Lấy mask feature đã chọn\n",
    "support_mask = rfe.support_          # mảng True/False\n",
    "ranking = rfe.ranking_               # 1 = giữ lại, >1 = bị loại\n",
    "\n",
    "# Nếu dùng pandas:\n",
    "selected_features = X_train.columns[support_mask]\n",
    "print(\"Các feature được chọn:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 5. Train model final trên tập feature đã chọn\n",
    "pos = (y_train==1).sum()\n",
    "neg = (y_train==0).sum()\n",
    "scale_pos_weight = neg / pos\n",
    "xgb_final = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=2,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "xgb_final.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# 6. Đánh giá\n",
    "y_pred = xgb_final.predict(X_test[selected_features])\n",
    "# print(\"y_test:\", np.asarray(y_test).shape, type(y_test))\n",
    "# print(\"y_pred:\", np.asarray(y_pred).shape, type(y_pred))\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0aefc7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: (576,) <class 'pandas.core.series.Series'>\n",
      "y_pred: (576,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"y_test:\", np.asarray(y_test).shape, type(y_test))\n",
    "print(\"y_pred:\", np.asarray(y_pred).shape, type(y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618d154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHẠY RFE VỚI SỐ LƯỢNG FEATURE SELECTION KHÁC NHAU (NHIỀU MODEL)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature counts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing with 45 features\n",
      "================================================================================\n",
      "  ✓ LR: Accuracy = 0.9497, F1 = 0.4871, AUC = 0.7533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature counts: 100%|██████████| 1/1 [00:08<00:00,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ LightGBM: Accuracy = 0.9514, F1 = 0.5986, AUC = 0.9141\n",
      "\n",
      "✅ Hoàn thành việc chạy tất cả các model với RFE!\n",
      "\n",
      "Tổng số kết quả: 2\n",
      "   n_features     model  accuracy        f1       auc\n",
      "0          45        LR  0.949653  0.487088  0.753325\n",
      "1          45  LightGBM  0.951389  0.598566  0.914140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class MLPWrapper:\n",
    "    \"\"\"Wrapper để MLPClassifier có thể dùng với RFE\"\"\"\n",
    "    def __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', \n",
    "                 max_iter=1000, random_state=42, **kwargs):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Trả về các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        return {\n",
    "            'hidden_layer_sizes': self.hidden_layer_sizes,\n",
    "            'activation': self.activation,\n",
    "            'solver': self.solver,\n",
    "            'max_iter': self.max_iter,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Thiết lập các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        # Tạo lại MLPClassifier với tham số mới\n",
    "        self.mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.mlp.fit(X, y)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Tính feature importance từ coefs_ (trọng số của layer đầu tiên)\n",
    "        if hasattr(self.mlp, 'coefs_') and len(self.mlp.coefs_) > 0:\n",
    "            # coefs_[0] có shape (n_neurons, n_features)\n",
    "            # Lấy giá trị tuyệt đối của trọng số và tính trung bình qua các neurons\n",
    "            coefs_first_layer = self.mlp.coefs_[0]\n",
    "            \n",
    "            # Đảm bảo số lượng features khớp\n",
    "            if coefs_first_layer.shape[1] == n_features:\n",
    "                self.feature_importances_ = np.abs(coefs_first_layer).mean(axis=0)\n",
    "            elif coefs_first_layer.shape[1] == n_features + 1:\n",
    "                # Nếu có bias term, bỏ cột cuối cùng\n",
    "                self.feature_importances_ = np.abs(coefs_first_layer[:, :-1]).mean(axis=0)\n",
    "            else:\n",
    "                # Nếu không khớp, dùng giá trị đều\n",
    "                self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        else:\n",
    "            # Nếu không có coefs_, dùng giá trị đều\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        # Đảm bảo feature_importances_ có đúng số lượng features\n",
    "        if len(self.feature_importances_) != n_features:\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.mlp.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.mlp.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.mlp.score(X, y)\n",
    "\n",
    "# Wrapper class để BaggingClassifier có thể dùng với RFE\n",
    "class BaggingWrapper:\n",
    "    \"\"\"Wrapper để BaggingClassifier có thể dùng với RFE\"\"\"\n",
    "    def __init__(self, estimator=None, n_estimators=10, random_state=42, **kwargs):\n",
    "        self.estimator = estimator if estimator is not None else RandomForestClassifier()\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.bagging = BaggingClassifier(\n",
    "            estimator=self.estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=random_state,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Trả về các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        return {\n",
    "            'estimator': self.estimator,\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Thiết lập các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        # Tạo lại BaggingClassifier với tham số mới\n",
    "        self.bagging = BaggingClassifier(\n",
    "            estimator=self.estimator,\n",
    "            n_estimators=self.n_estimators,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.bagging.fit(X, y)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Tính feature importance từ các base estimators\n",
    "        if hasattr(self.bagging, 'estimators_') and len(self.bagging.estimators_) > 0:\n",
    "            importances = []\n",
    "            for estimator in self.bagging.estimators_:\n",
    "                if hasattr(estimator, 'feature_importances_'):\n",
    "                    imp = estimator.feature_importances_\n",
    "                    # Đảm bảo số lượng features khớp\n",
    "                    if len(imp) == n_features:\n",
    "                        importances.append(imp)\n",
    "                elif hasattr(estimator, 'coef_'):\n",
    "                    coef = estimator.coef_\n",
    "                    # Xử lý coef_ có thể có shape khác nhau\n",
    "                    if coef.ndim == 1:\n",
    "                        imp = np.abs(coef)\n",
    "                    elif coef.ndim == 2:\n",
    "                        imp = np.abs(coef[0])\n",
    "                    else:\n",
    "                        imp = np.abs(coef).mean(axis=0)\n",
    "                    \n",
    "                    # Đảm bảo số lượng features khớp\n",
    "                    if len(imp) == n_features:\n",
    "                        importances.append(imp)\n",
    "                    elif len(imp) == n_features + 1:\n",
    "                        # Nếu có bias term, bỏ phần tử cuối\n",
    "                        importances.append(imp[:-1])\n",
    "            \n",
    "            if importances:\n",
    "                # Đảm bảo tất cả importances có cùng shape\n",
    "                importances = [imp for imp in importances if len(imp) == n_features]\n",
    "                if importances:\n",
    "                    self.feature_importances_ = np.mean(importances, axis=0)\n",
    "                else:\n",
    "                    self.feature_importances_ = np.ones(n_features) / n_features\n",
    "            else:\n",
    "                self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        else:\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        # Đảm bảo feature_importances_ có đúng số lượng features\n",
    "        if len(self.feature_importances_) != n_features:\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.bagging.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.bagging.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.bagging.score(X, y)\n",
    "\n",
    "# Hàm tạo model instances mới cho RFE\n",
    "def get_rfe_estimator(model_name):\n",
    "    \"\"\"Tạo estimator mới cho RFE\"\"\"\n",
    "    \n",
    "    if model_name == 'LR':\n",
    "        return LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42, class_weight=\"balanced\")\n",
    "    elif model_name == 'LightGBM':\n",
    "        return LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42, verbose=-1, scale_pos_weight=scale_pos_weight)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "# Hàm tạo model instances mới cho training\n",
    "def get_model_instance(model_name):\n",
    "    \"\"\"Tạo instance mới của model để tránh dùng lại model đã được fit\"\"\"\n",
    "    if model_name == 'LR':\n",
    "        return LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "    elif model_name == 'LightGBM':\n",
    "        return LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42, verbose=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "# Định nghĩa các số lượng features cần test\n",
    "feature_counts = [114]\n",
    "\n",
    "# Lưu trữ kết quả\n",
    "results_by_features = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHẠY RFE VỚI SỐ LƯỢNG FEATURE SELECTION KHÁC NHAU (NHIỀU MODEL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "X_features = X_base_features.copy()\n",
    "y = y_base.copy()\n",
    "\n",
    "# Chia train-test một lần để đảm bảo tính nhất quán\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_features, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Danh sách các model cần test\n",
    "model_names = ['LR', 'LightGBM']\n",
    "\n",
    "# Vòng lặp qua các số lượng features\n",
    "for n_features in tqdm(feature_counts, desc=\"Processing feature counts\"):\n",
    "    if n_features > X_features.shape[1]:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing with {n_features} features\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Chạy từng model\n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            # Chạy RFE feature selection với estimator tương ứng\n",
    "            rfe_estimator = get_rfe_estimator(model_name)\n",
    "            \n",
    "            rfe = RFE(\n",
    "                estimator=rfe_estimator,\n",
    "                n_features_to_select=n_features,\n",
    "                step=1\n",
    "            )\n",
    "            \n",
    "            # Fit selector trên training set\n",
    "            rfe.fit(X_train_full, y_train_full)\n",
    "            X_train_selected = X_train_full.loc[:, rfe.support_]\n",
    "            X_test_selected = X_test_full.loc[:, rfe.support_]\n",
    "            \n",
    "            # Tạo model instance mới cho training\n",
    "            model = get_model_instance(model_name)\n",
    "            \n",
    "            # Train và evaluate\n",
    "            model.fit(X_train_selected, y_train_full)\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "            accuracy = accuracy_score(y_test_full, y_pred)\n",
    "            \n",
    "            # Tính F1 score\n",
    "            f1 = f1_score(y_test_full, y_pred, average='macro')\n",
    "            \n",
    "            # Tính AUC\n",
    "            try:\n",
    "                y_pred_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "                auc = roc_auc_score(y_test_full, y_pred_proba, multi_class='ovr', average='macro')\n",
    "            except:\n",
    "                auc = np.nan\n",
    "            \n",
    "            results_by_features.append({\n",
    "                'n_features': n_features,\n",
    "                'model': model_name,\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'auc': auc\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✓ {model_name}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC = {auc:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error with {model_name} at {n_features} features: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n✅ Hoàn thành việc chạy tất cả các model với RFE!\")\n",
    "\n",
    "# Chuyển đổi kết quả thành DataFrame\n",
    "results_df_features = pd.DataFrame(results_by_features)\n",
    "print(f\"\\nTổng số kết quả: {len(results_df_features)}\")\n",
    "print(results_df_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e11539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHẠY RFE VỚI SỐ LƯỢNG FEATURE SELECTION KHÁC NHAU (NHIỀU MODEL)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature counts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing with 45 features\n",
      "================================================================================\n",
      "  ✓ LR: Accuracy = 0.9497, F1 = 0.4871, AUC = 0.7533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature counts: 100%|██████████| 1/1 [00:08<00:00,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ LightGBM: Accuracy = 0.9514, F1 = 0.5986, AUC = 0.9141\n",
      "\n",
      "✅ Hoàn thành việc chạy tất cả các model với RFE!\n",
      "\n",
      "Tổng số kết quả: 2\n",
      "   n_features     model  accuracy        f1       auc\n",
      "0          45        LR  0.949653  0.487088  0.753325\n",
      "1          45  LightGBM  0.951389  0.598566  0.914140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class MLPWrapper:\n",
    "    \"\"\"Wrapper để MLPClassifier có thể dùng với RFE\"\"\"\n",
    "    def __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', \n",
    "                 max_iter=1000, random_state=42, **kwargs):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Trả về các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        return {\n",
    "            'hidden_layer_sizes': self.hidden_layer_sizes,\n",
    "            'activation': self.activation,\n",
    "            'solver': self.solver,\n",
    "            'max_iter': self.max_iter,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Thiết lập các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        # Tạo lại MLPClassifier với tham số mới\n",
    "        self.mlp = MLPClassifier(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.mlp.fit(X, y)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Tính feature importance từ coefs_ (trọng số của layer đầu tiên)\n",
    "        if hasattr(self.mlp, 'coefs_') and len(self.mlp.coefs_) > 0:\n",
    "            # coefs_[0] có shape (n_neurons, n_features)\n",
    "            # Lấy giá trị tuyệt đối của trọng số và tính trung bình qua các neurons\n",
    "            coefs_first_layer = self.mlp.coefs_[0]\n",
    "            \n",
    "            # Đảm bảo số lượng features khớp\n",
    "            if coefs_first_layer.shape[1] == n_features:\n",
    "                self.feature_importances_ = np.abs(coefs_first_layer).mean(axis=0)\n",
    "            elif coefs_first_layer.shape[1] == n_features + 1:\n",
    "                # Nếu có bias term, bỏ cột cuối cùng\n",
    "                self.feature_importances_ = np.abs(coefs_first_layer[:, :-1]).mean(axis=0)\n",
    "            else:\n",
    "                # Nếu không khớp, dùng giá trị đều\n",
    "                self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        else:\n",
    "            # Nếu không có coefs_, dùng giá trị đều\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        # Đảm bảo feature_importances_ có đúng số lượng features\n",
    "        if len(self.feature_importances_) != n_features:\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.mlp.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.mlp.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.mlp.score(X, y)\n",
    "\n",
    "# Wrapper class để BaggingClassifier có thể dùng với RFE\n",
    "class BaggingWrapper:\n",
    "    \"\"\"Wrapper để BaggingClassifier có thể dùng với RFE\"\"\"\n",
    "    def __init__(self, estimator=None, n_estimators=10, random_state=42, **kwargs):\n",
    "        self.estimator = estimator if estimator is not None else RandomForestClassifier()\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.bagging = BaggingClassifier(\n",
    "            estimator=self.estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=random_state,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Trả về các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        return {\n",
    "            'estimator': self.estimator,\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Thiết lập các tham số của estimator (cần cho sklearn clone)\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        # Tạo lại BaggingClassifier với tham số mới\n",
    "        self.bagging = BaggingClassifier(\n",
    "            estimator=self.estimator,\n",
    "            n_estimators=self.n_estimators,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.bagging.fit(X, y)\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Tính feature importance từ các base estimators\n",
    "        if hasattr(self.bagging, 'estimators_') and len(self.bagging.estimators_) > 0:\n",
    "            importances = []\n",
    "            for estimator in self.bagging.estimators_:\n",
    "                if hasattr(estimator, 'feature_importances_'):\n",
    "                    imp = estimator.feature_importances_\n",
    "                    # Đảm bảo số lượng features khớp\n",
    "                    if len(imp) == n_features:\n",
    "                        importances.append(imp)\n",
    "                elif hasattr(estimator, 'coef_'):\n",
    "                    coef = estimator.coef_\n",
    "                    # Xử lý coef_ có thể có shape khác nhau\n",
    "                    if coef.ndim == 1:\n",
    "                        imp = np.abs(coef)\n",
    "                    elif coef.ndim == 2:\n",
    "                        imp = np.abs(coef[0])\n",
    "                    else:\n",
    "                        imp = np.abs(coef).mean(axis=0)\n",
    "                    \n",
    "                    # Đảm bảo số lượng features khớp\n",
    "                    if len(imp) == n_features:\n",
    "                        importances.append(imp)\n",
    "                    elif len(imp) == n_features + 1:\n",
    "                        # Nếu có bias term, bỏ phần tử cuối\n",
    "                        importances.append(imp[:-1])\n",
    "            \n",
    "            if importances:\n",
    "                # Đảm bảo tất cả importances có cùng shape\n",
    "                importances = [imp for imp in importances if len(imp) == n_features]\n",
    "                if importances:\n",
    "                    self.feature_importances_ = np.mean(importances, axis=0)\n",
    "                else:\n",
    "                    self.feature_importances_ = np.ones(n_features) / n_features\n",
    "            else:\n",
    "                self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        else:\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        # Đảm bảo feature_importances_ có đúng số lượng features\n",
    "        if len(self.feature_importances_) != n_features:\n",
    "            self.feature_importances_ = np.ones(n_features) / n_features\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.bagging.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.bagging.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.bagging.score(X, y)\n",
    "\n",
    "# Hàm tạo model instances mới cho RFE\n",
    "def get_rfe_estimator(model_name):\n",
    "    \"\"\"Tạo estimator mới cho RFE\"\"\"\n",
    "    \n",
    "    if model_name == 'LR':\n",
    "        return LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42, class_weight=\"balanced\")\n",
    "    elif model_name == 'LightGBM':\n",
    "        return LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42, verbose=-1, scale_pos_weight=scale_pos_weight)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "# Hàm tạo model instances mới cho training\n",
    "def get_model_instance(model_name):\n",
    "    \"\"\"Tạo instance mới của model để tránh dùng lại model đã được fit\"\"\"\n",
    "    if model_name == 'LR':\n",
    "        return LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "    elif model_name == 'LightGBM':\n",
    "        return LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42, verbose=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "# Định nghĩa các số lượng features cần test\n",
    "feature_counts = [114]\n",
    "\n",
    "# Lưu trữ kết quả\n",
    "results_by_features = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHẠY RFE VỚI SỐ LƯỢNG FEATURE SELECTION KHÁC NHAU (NHIỀU MODEL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "X_features = X_base_features.copy()\n",
    "y = y_base.copy()\n",
    "\n",
    "# Chia train-test một lần để đảm bảo tính nhất quán\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_features, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Danh sách các model cần test\n",
    "model_names = ['LR', 'LightGBM']\n",
    "\n",
    "# Vòng lặp qua các số lượng features\n",
    "for n_features in tqdm(feature_counts, desc=\"Processing feature counts\"):\n",
    "    if n_features > X_features.shape[1]:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing with {n_features} features\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Chạy từng model\n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            # Chạy RFE feature selection với estimator tương ứng\n",
    "            rfe_estimator = get_rfe_estimator(model_name)\n",
    "            \n",
    "            rfe = RFE(\n",
    "                estimator=rfe_estimator,\n",
    "                n_features_to_select=n_features,\n",
    "                step=1\n",
    "            )\n",
    "            \n",
    "            # Fit selector trên training set\n",
    "            rfe.fit(X_train_full, y_train_full)\n",
    "            X_train_selected = X_train_full.loc[:, rfe.support_]\n",
    "            X_test_selected = X_test_full.loc[:, rfe.support_]\n",
    "            \n",
    "            # Tạo model instance mới cho training\n",
    "            model = get_model_instance(model_name)\n",
    "            \n",
    "            # Train và evaluate\n",
    "            model.fit(X_train_selected, y_train_full)\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "            accuracy = accuracy_score(y_test_full, y_pred)\n",
    "            \n",
    "            # Tính F1 score\n",
    "            f1 = f1_score(y_test_full, y_pred, average='macro')\n",
    "            \n",
    "            # Tính AUC\n",
    "            try:\n",
    "                y_pred_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "                auc = roc_auc_score(y_test_full, y_pred_proba, multi_class='ovr', average='macro')\n",
    "            except:\n",
    "                auc = np.nan\n",
    "            \n",
    "            results_by_features.append({\n",
    "                'n_features': n_features,\n",
    "                'model': model_name,\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'auc': auc\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✓ {model_name}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC = {auc:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error with {model_name} at {n_features} features: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n✅ Hoàn thành việc chạy tất cả các model với RFE!\")\n",
    "\n",
    "# Chuyển đổi kết quả thành DataFrame\n",
    "results_df_features = pd.DataFrame(results_by_features)\n",
    "print(f\"\\nTổng số kết quả: {len(results_df_features)}\")\n",
    "print(results_df_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db70f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(random_state=42, verbose=-1)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c92ca36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LR': LogisticRegression(class_weight='balanced', max_iter=1000, penalty='l2',\n",
      "                   random_state=42), 'LightGBM': LGBMClassifier(is_unbalance=True, random_state=42, verbose=-1)}\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451b596",
   "metadata": {},
   "source": [
    "# 📊 INFERENCE & PREDICTION\n",
    "\n",
    "Phần này sẽ:\n",
    "1. Load dữ liệu test\n",
    "2. Preprocessing (giống training)\n",
    "3. Dự đoán với model tốt nhất\n",
    "4. Tạo file CSV submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d5150e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING TEST DATA\n",
      "============================================================\n",
      "Test data shape: (7135, 116)\n",
      "\n",
      "First 3 rows:\n",
      "                  object_id       Z    Z_err    EBV  u_n_obs  u_time_span  \\\n",
      "0  Eluwaith_Mithrim_nothrim  0.5393  0.03013  0.610        2    1216.0901   \n",
      "1        Eru_heledir_archam  0.7282  0.03508  0.058        9    1830.1833   \n",
      "2         Gonhir_anann_fuin  0.6026  0.03185  0.070       17    2359.4595   \n",
      "\n",
      "   u_flux_mean  u_flux_std  u_flux_min  u_flux_max  ...  color_mean_g_i  \\\n",
      "0     0.080995    0.063499    0.017497    0.144494  ...       -2.848042   \n",
      "1     0.318856    0.438312   -0.472787    1.009501  ...       -0.941344   \n",
      "2     0.154386    0.809561   -1.896650    1.650006  ...       -0.181608   \n",
      "\n",
      "   color_mean_r_z  t_peak_diff_u_g  t_peak_diff_g_r  t_peak_diff_r_i  \\\n",
      "0       -2.747490         -28.9546        -636.9996         242.7726   \n",
      "1       -0.350325         411.1488         -11.4208         702.3793   \n",
      "2        0.047289        -212.9875        -652.7930         -38.7250   \n",
      "\n",
      "   t_peak_diff_i_z  t_peak_diff_z_y  t_peak_diff_u_r  t_peak_diff_g_i  \\\n",
      "0           6.6818              NaN        -665.9542        -394.2270   \n",
      "1          48.5384        -119.9184         399.7280         690.9585   \n",
      "2        1385.8021       -2104.9809        -865.7805        -691.5180   \n",
      "\n",
      "   t_peak_diff_r_z  \n",
      "0         249.4544  \n",
      "1         750.9177  \n",
      "2        1347.0771  \n",
      "\n",
      "[3 rows x 116 columns]\n",
      "\n",
      "Columns: ['object_id', 'Z', 'Z_err', 'EBV', 'u_n_obs', 'u_time_span', 'u_flux_mean', 'u_flux_std', 'u_flux_min', 'u_flux_max']...\n",
      "\n",
      "Checking for NaN:\n",
      "Total NaN: 8881\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD TEST DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv('/home/duy/Downloads/Mallorn_update/Mallorn/mallorn-astronomical-classification-challenge/test_features_ml.csv')\n",
    "\n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_test.head(3))\n",
    "print(f\"\\nColumns: {df_test.columns.tolist()[:10]}...\")\n",
    "print(f\"\\nChecking for NaN:\")\n",
    "print(f\"Total NaN: {df_test.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9d05d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPROCESSING TEST DATA\n",
      "============================================================\n",
      "\n",
      "2. Điền MEDIAN vào các cột số còn lại...\n",
      "   ✓ Đã điền MEDIAN vào 91 cột\n",
      "\n",
      "3. Saved 7135 object IDs for submission\n",
      "\n",
      "4. Scaling features...\n",
      "   Columns to scale: 114\n",
      "   ✓ Scaled using training scaler\n",
      "\n",
      "5. Preparing features...\n",
      "   ✓ Test features shape: (7135, 114)\n",
      "\n",
      "============================================================\n",
      "✓ TEST DATA READY FOR INFERENCE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PREPROCESS TEST DATA (SAME AS TRAINING)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "all_results = []\n",
    "# # 1. Fill Z_err with 0\n",
    "# print(\"1. Filling Z_err with 0...\")\n",
    "# if 'Z_err' in df_test.columns:\n",
    "#     df_test['Z_err'] = df_test['Z_err'].fillna(0)\n",
    "#     print(f\"   ✓ Filled Z_err\")\n",
    "\n",
    "# 2. Drop rows with NaN in other columns\n",
    "# print(\"\\n2. Dropping rows with NaN in other columns...\")\n",
    "# rows_before = len(df_test)\n",
    "# df_test_cleaned = df_test.dropna()\n",
    "# rows_after = len(df_test_cleaned)\n",
    "# print(f\"   ✓ Dropped {rows_before - rows_after} rows ({(rows_before - rows_after)/rows_before*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# 2. Điền MEDIAN vào các cột số còn lại\n",
    "print(\"\\n2. Điền MEDIAN vào các cột số còn lại...\")\n",
    "numerical_cols = df_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['Z_err', 'target']]\n",
    "\n",
    "filled_count = 0\n",
    "for col in numerical_cols:\n",
    "    if df_test[col].isnull().sum() > 0:\n",
    "        median_value = df_test[col].median()\n",
    "        df_test[col] = df_test[col].fillna(median_value)\n",
    "        filled_count += 1\n",
    "\n",
    "print(f\"   ✓ Đã điền MEDIAN vào {filled_count} cột\")\n",
    "\n",
    "# 3. Save object_id for submission\n",
    "test_object_ids = df_test['object_id'].copy()\n",
    "print(f\"\\n3. Saved {len(test_object_ids)} object IDs for submission\")\n",
    "\n",
    "# 4. Scale the features (using the SAME scaler from training)\n",
    "print(\"\\n4. Scaling features...\")\n",
    "df_test_scaled = df_test.copy()\n",
    "\n",
    "# Get columns to scale (same as training)\n",
    "columns_to_scale_test = [col for col in df_test_scaled.columns \n",
    "                          if col not in ['object_id', 'Z_err'] and col in columns_to_scale]\n",
    "\n",
    "print(f\"   Columns to scale: {len(columns_to_scale_test)}\")\n",
    "\n",
    "# Apply the SAME scaler from training data\n",
    "df_test_scaled[columns_to_scale_test] = scaler.transform(df_test_scaled[columns_to_scale_test])\n",
    "print(f\"   ✓ Scaled using training scaler\")\n",
    "\n",
    "# 5. Prepare features (same as training)\n",
    "print(\"\\n5. Preparing features...\")\n",
    "exclude_cols_test = ['object_id', 'Z_err']\n",
    "feature_cols_test = [col for col in df_test_scaled.columns \n",
    "                     if col not in exclude_cols_test \n",
    "                     and df_test_scaled[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "X_test_features = df_test_scaled[feature_cols_test].copy()\n",
    "\n",
    "print(f\"   ✓ Test features shape: {X_test_features.shape}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TEST DATA READY FOR INFERENCE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# df_test = df_test.copy()\n",
    "\n",
    "# # 1. Điền 0 vào cột Z_err\n",
    "# print(\"1. Điền giá trị 0 vào cột Z_err...\")\n",
    "# df_test['Z_err'] = df_test['Z_err'].fillna(0)\n",
    "# print(f\"   ✓ Đã điền 0 vào {nan_counts['Z_err']} giá trị NaN trong Z_err\")\n",
    "\n",
    "# # 2. Điền MEDIAN vào các cột số còn lại\n",
    "# print(\"\\n2. Điền MEDIAN vào các cột số còn lại...\")\n",
    "# numerical_cols = df_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "# numerical_cols = [col for col in numerical_cols if col not in ['Z_err', 'target']]\n",
    "\n",
    "# filled_count = 0\n",
    "# for col in numerical_cols:\n",
    "#     if df_test[col].isnull().sum() > 0:\n",
    "#         median_value = df_test[col].median()\n",
    "#         df_test[col] = df_test[col].fillna(median_value)\n",
    "#         filled_count += 1\n",
    "\n",
    "# print(f\"   ✓ Đã điền MEDIAN vào {filled_count} cột\")\n",
    "\n",
    "# # 3. Kiểm tra xem còn NaN không\n",
    "# print(\"\\n3. Kiểm tra lại...\")\n",
    "# remaining_nan = df_test.isnull().sum().sum()\n",
    "# if remaining_nan > 0:\n",
    "#     print(f\"   ⚠️ Còn {remaining_nan} NaN (có thể ở cột text)\")\n",
    "#     object_cols = df_test.select_dtypes(include=['object']).columns\n",
    "#     for col in object_cols:\n",
    "#         if col != 'object_id':\n",
    "#             df_test[col] = df_test[col].fillna('Unknown')\n",
    "#     print(\"   ✓ Đã điền 'Unknown' vào các cột text\")\n",
    "# else:\n",
    "#     print(\"   ✓ Không còn NaN\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"SAU KHI XỬ LÝ NaN\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"Kích thước DataFrame: {df_test.shape} (KHÔNG XÓA HÀNG ✓)\")\n",
    "# print(\"Số hàng đã xóa: 0\")\n",
    "# print(f\"\\nCòn NaN trong DataFrame không? {df_test.isnull().any().any()}\")\n",
    "# print(f\"Tổng số NaN còn lại: {df_test.isnull().sum().sum()}\")\n",
    "\n",
    "# # Hiển thị thống kê cột Z_err sau khi điền\n",
    "# print(\"\\nThống kê cột Z_err sau khi điền 0:\")\n",
    "# print(f\"  - Min: {df_test['Z_err'].min()}\")\n",
    "# print(f\"  - Max: {df_test['Z_err'].max()}\")\n",
    "# print(f\"  - Mean: {df_test['Z_err'].mean():.4f}\")\n",
    "# print(f\"  - Số giá trị = 0: {(df_test['Z_err'] == 0).sum()}\")\n",
    "\n",
    "# # Cập nhật df với bản đã điền\n",
    "# df = df_test.copy()\n",
    "\n",
    "# print(\"\\n✓ DataFrame đã được cập nhật!\")\n",
    "# print(f\"✓ Kích thước hiện tại: {df.shape}\")\n",
    "# print(f\"✓ Đã GIỮ NGUYÊN tất cả {len(df)} hàng dữ liệu!\")\n",
    "\n",
    "# X_test_features = df_test_scaled[feature_cols_test].copy()\n",
    "\n",
    "# print(f\"   ✓ Test features shape: {X_test_features.shape}\")\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"✓ TEST DATA READY FOR INFERENCE!\")\n",
    "# print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2a5bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using threshold = 0.002\n",
      "(7135, 45)\n",
      "✓ Submission file created: /home/duy/Downloads/Mallorn_update/Mallorn/submission.csv\n",
      "Prediction distribution:\n",
      "prediction\n",
      "0    7126\n",
      "1       9\n",
      "Name: count, dtype: int64\n",
      "proba1 min/mean/max: 8.54004514464648e-06 0.0001559921464739057 0.005262088976311692\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# ============================================================\n",
    "# 1) Chọn đúng cột (đúng thứ tự) + check thiếu cột\n",
    "# ============================================================\n",
    "selected_features = list(selected_features)\n",
    "\n",
    "missing = [c for c in selected_features if c not in X_test_features.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"X_test_features thiếu các cột: {missing}\")\n",
    "\n",
    "X_kaggle_test_sel = X_test_features.loc[:, selected_features].copy()\n",
    "\n",
    "# ============================================================\n",
    "# 2) TÌM THRESHOLD TỐT NHẤT TRÊN VALIDATION (F1)\n",
    "#    Bạn cần có (X_val_features, y_val) hoặc dùng holdout (X_test, y_test) của bạn.\n",
    "#    Nếu notebook bạn đang gọi tập holdout là X_test/y_test thì thay bằng biến đó.\n",
    "# ============================================================\n",
    "def best_f1_threshold(y_true, proba1):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, proba1)\n",
    "    # thresholds có length = len(precision)-1\n",
    "    f1 = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1))\n",
    "    return float(thresholds[best_idx]), float(f1[best_idx])\n",
    "\n",
    "# Ví dụ: dùng holdout validation của bạn (đổi tên biến cho đúng notebook)\n",
    "# X_val_sel = X_test.loc[:, selected_features].copy()\n",
    "# proba_val = model.predict_proba(X_val_sel)[:, 1]\n",
    "# thr, best_f1 = best_f1_threshold(y_test.values, proba_val)\n",
    "\n",
    "# Nếu bạn chưa có validation riêng, tạm dùng threshold “nhẹ” để tránh all-0:\n",
    "thr = 0.002  # bạn có thể thử 0.1, 0.05 nếu lớp 1 cực hiếm\n",
    "\n",
    "print(\"Using threshold =\", thr)\n",
    "print(X_kaggle_test_sel.shape)\n",
    "# ============================================================\n",
    "# 3) Predict PROBA trên Kaggle test + áp threshold -> label\n",
    "# ============================================================\n",
    "# proba_test = model.predict_proba(X_test_features)[:, 1]\n",
    "proba_test = model.predict_proba(X_kaggle_test_sel)[:, 1]\n",
    "\n",
    "y_pred_test = (proba_test >= thr).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 4) Create submission\n",
    "# ============================================================\n",
    "submission = pd.DataFrame({\n",
    "    \"object_id\": test_object_ids,\n",
    "    \"prediction\": y_pred_test\n",
    "})\n",
    "\n",
    "submission_path = \"/home/duy/Downloads/Mallorn_update/Mallorn/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✓ Submission file created: {submission_path}\")\n",
    "print(\"Prediction distribution:\")\n",
    "print(submission[\"prediction\"].value_counts())\n",
    "print(\"proba1 min/mean/max:\", proba_test.min(), proba_test.mean(), proba_test.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a964d92",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
